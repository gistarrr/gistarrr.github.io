---
title:  "부스트캠프 AI Tech 1주차 주간 학습 정리"
excerpt: "Level 1 Ustage 1주차"
categories:
  - Ustage
tags:
  - [ustage]
toc: true
toc_sticky: true
date: 2021-08-02
author_profile: false
last_modified_at: 2021-08-02
---
**Day 1**
===

- 학습 강의 : Python Basic for AI 1-1 ~ 3-2강
- 과제 : 필수과제 1, 2, 3  

## 1. 강의 요약 및 새로 알게된 내용
- 개발환경 세팅 : Jupyter notebook, Colab 
  - Github의 ipynb파일의 URL에서 gitbub.com -> github로 변경하고 colab.research.google.com앞에 추가
- 개인적으로 Github Blog 공부 후 생성
- Numpy의 np.median() 함수
  - array의 길이가 홀수일 경우 중앙값 1개, 짝수일 경우 중앙값 2개의 평균을 반환
- Pre-course에서 이미 한 번 들었기 때문에 간단하게 복습

## 2. 고민한 내용
- Peer session을 더 효율적으로 할 수 있는 방법

## 3. 참고할 만한 자료
- [Numpy 중앙값](https://mindscale.kr/course/basic-stat-python/3/){:target="_blank"}

## 4. 피어세션
- HackerMD를 사용하여 회의록 공동 작성
- 피어끼리 Github Group을 만들고 레퍼지토리의 Wiki에 회의록 업로드
- 자세한 내용은 [MeetUP & day 1](https://github.com/round26/round26/wiki/Week1-Day1){:target="_blank"} 참조

## 5. 회고
- 이미 배웠던 것들을 다시 들으면서 처음 강의 들을 때는 무조건 기억할 거라고 자신했던 나를 반성하고... 이번 기회에 내 머리는 그리 좋지 못하다는 것을 파악하였고 복습의 중요성을 깨달았다. 깃허브 블로그와 마크다운 사용법을 익히고 이를 활용하여 당일 배운 것을 복습하여 정리하는 습관을 들이도록 노력해야 할 것 같다.
- 멘토 소개 시간에서 좋은 말씀을 해주셨다.
  > 당신이 가는 곳이 다 길이다.  
- 내가 가는 길을 의심하지 않고 꾸준히 걸어가보려고 한다.

---

**Day 2**
===

- 학습 강의 : Python Basic for AI 4-1 ~ 4-2강, AI Math 1 ~ 3강
- 과제 : 필수과제 4, 5 & 필수 퀴즈 1 ~ 4강 
<br>

## 1. 강의 요약
## 2. 고민한 내용
## 3. 참고할 만한 자료
## 4. 회고
---

**Day 3**
===

- 학습 강의 : Python Basic for AI 5-1 ~ 5-2강, AI Math 5 ~ 6강
- 과제 : 필수퀴즈 5, 6강, 선택과제 진행

## 1. 강의 복습
### Python Basic for AI
5-1강 : File/Exception/Log handling
- 적자    

5-2강 : Python data handling
- 적자

### AI Math
5강 : 딥러닝 학습 방법 이해하기
- 비선형모델인 neural network의 원리
- softmax함수를 통해 선형모델을 확률벡터로 변환할 수 있다.
  - 분류 문제를 풀 때 선형모델과 softmax 함수를 결합하여 해결할 수 있다.
  - 추론의 경우 softmax함수가 아닌 one-hot 벡터를 이용
- 신경망은 선형모델과 활성함수를 합성한 함수이다. 이 때 활성함수는 하나의 실수값을 input으로 받는다.
  - 벡터나 행렬이 들어온다면 각각의 요소에 활성함수를 적용한다. 즉, 개별적으로 함수를 적용해 새로운 히든 벡터를 생성한다. 히든 벡터를 뉴런이라고 부른다. 
- 활성함수는 실수 범위에서 정의된 비선형 함수를 의미한다.
- 가중치 행렬과 활성함수를 여러 번 합성하게 되면 이것이 신경망이 되고 이를 순전파(forward propagation)라 한다.
- 역전파 알고리즘 : 경사하강법을 적용해 각각의 가중치 행렬들을 학습, 이 때 가중치 행렬들의 Gradient Vector를 계산해야 한다.
  - 연쇄법칙이 기반이 된다.  

6강 : 확률론 맛보기
- 데이터에서 관찰되는 분포와 모델 예측의 분포의 차이를 최소화하도록 학습한다. 즉, 확률론 기반으로 해석이 가능하다.
- 확률변수는 함수이다.
- 이산형 확률변수는 확률변수에 값을 가지는 모든 확률을 더해서 모델링하고 연속형 확률변수는 적분을 통해서 모델링한다. 
- 조건부 확률에 대한 설명
  - 데이터 x가 주어질 때 선형모델과 결합하여 softmax함수를 이용하면 확률벡터를 구할 수 있다.
- 기대값에 대한 설명
  - 이산형 확률 변수는 질량함수를 확률과 곱하여 급수를 취하고 연속형 확률 변수는 밀도함수 곱하여 적분을 취해 기대값을 구한다.
- 확률분포를 모를 때에는 몬테카를로 샘플링 방법을 사용 -> i.i.d(identical independent distributed)를 만족하는 확률변수여야 한다. 

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- 강의나 퀴즈는 Precourse에서 이미 한 번 했기 때문에 복습하는 차원에서 진행하였다.  

## 3. 참고할 만한 자료
- [Statistics110](https://www.boostcourse.org/ai152/joinLectures/195039)

## 4. 피어세션
- 필수퀴즈의 베이지안 공식에 대해 질문을 하고 답변을 받았다.
- 필수과제 4, 5에 대한 코드리뷰를 진행
- 자세한 내용은 [Week1_day3](https://github.com/round26/round26/wiki/Week-1-Day-3){:target="_blank"} 참조

## 5. 회고
- 다음부터는 아이패드 굿노트와 필기한 것을 활용하여 이미지도 첨부하여 글을 정리하면 좋을 것 같다.
- 오늘 멘토링을 진행했는데 스마트 팩토리, 스타트업에 대한 설명과 목적 의식, 동기 부여에 대한 좋은 조언을 얻었다. 그리고 가능하다면 캐글을 병행하는 게 많은 도움이 될 것이라고 하셨다. 그렇게 되게끔 만들어보자!

---

**Day 4**
===

---

**Day 5**
===