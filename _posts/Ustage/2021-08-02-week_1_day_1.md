---
title:  "부스트캠프 AI Tech 1주차 주간 학습 정리"
excerpt: "Level 1 Ustage 1주차"
categories:
  - Ustage
tags:
  - [ustage]
toc: true
toc_sticky: false
date: 2021-08-02
author_profile: false
last_modified_at: 2021-08-02
---
**Day 1**
===

- 학습 강의 : Python Basic for AI 1-1 ~ 3-2강
- 과제 : 필수과제 1, 2, 3  

## 1. 강의 요약 및 새로 알게된 내용
- 개발환경 세팅 : Jupyter notebook, Colab 
  - Github의 ipynb파일의 URL에서 gitbub.com -> github로 변경하고 colab.research.google.com앞에 추가
- 개인적으로 Github Blog 공부 후 생성
- Numpy의 np.median() 함수
  - array의 길이가 홀수일 경우 중앙값 1개, 짝수일 경우 중앙값 2개의 평균을 반환
- Pre-course에서 이미 한 번 들었기 때문에 간단하게 복습

## 2. 고민한 내용
- Peer session을 더 효율적으로 할 수 있는 방법

## 3. 참고할 만한 자료
- [Numpy 중앙값](https://mindscale.kr/course/basic-stat-python/3/){:target="_blank"}

## 4. 피어세션
- HackerMD를 사용하여 회의록 공동 작성
- 피어끼리 Github Group을 만들고 레퍼지토리의 Wiki에 회의록 업로드
- 자세한 내용은 [MeetUP & day 1](https://github.com/round26/round26/wiki/Week1-Day1){:target="_blank"} 참조

## 5. 회고
- 이미 배웠던 것들을 다시 들으면서 처음 강의 들을 때는 무조건 기억할 거라고 자신했던 나를 반성하고... 이번 기회에 내 머리는 그리 좋지 못하다는 것을 파악하였고 복습의 중요성을 깨달았다. 깃허브 블로그와 마크다운 사용법을 익히고 이를 활용하여 당일 배운 것을 복습하여 정리하는 습관을 들이도록 노력해야 할 것 같다.
- 멘토 소개 시간에서 좋은 말씀을 해주셨다.
  > 당신이 가는 곳이 다 길이다.  
- 내가 가는 길을 의심하지 않고 꾸준히 걸어가보려고 한다.

---
---

**Day 2**
===

- 학습 강의 : Python Basic for AI 4-1 ~ 4-2강, AI Math 1 ~ 4강
- 과제 : 필수과제 4, 5 & 필수 퀴즈 1 ~ 4강 
<br>

## 1. 강의 요약 및 새로 알게된 내용
- 파이썬 함수를 모듈화하는 방법
- 파이썬 decorator에 관한 내용
- x가 2차원 벡터이면 2차원 평면 상의 점으로 볼 수 있다. -> 이를 확장하면 n차원 벡터는 n차원 평면 상의 점으로 볼 수 있다.
- 입력 데이터가 n x m 행렬로 주어진다면 m차원 벡터가 n개 존재하는 것으로 해석 -> m차원 벡터는 m차원 상의 점으로 이를 하나의 데이터로 간주할 수 있고 이러한 데이터가 n개 존재한다.
- 경사하강법 기본 원리

## 2. 고민한 내용
- 경사하강법에서 선형회귀 목적식을 직접 미분해봤는데 정확하게 유도가 되지 않았다.
- 이번 필수과제 4에서 main 함수 짜는데 많은 시간이 걸렸다. 종료 조건을 설정하는데 표현 상 애매한 부분이 있었던 것 같다. 그리고 어떻게 하면 while문의 반복 없이 깔끔하게 짤 수 있을까 고민을 했다. ~~(고민만 했다)~~

## 3. 참고할 만한 자료
- [파이썬 데코레이터](https://dojang.io/mod/page/view.php?id=2427)

## 4. 피어세션
- 필수과제 1, 2, 3에 대한 코드 리뷰를 진행하였다.
- 생각보다 강의의 양이 많아 그날 그날 계획을 수정하기로 했다.
- 자세한 내용은 [Week1_day2](https://github.com/round26/round26/wiki/Week1-Day2){:target="_blank"} 참조

## 5. 회고
- 코스 시작 전에 들었을 때에는 잘 이해가 되지 않았는데 다시 들어보니깐 이해가 되는 부분이 많았다. 모르는 부분이 있더라도 반복해서 보면 언젠간 이해가 되지 않을까? 

---
---

**Day 3**
===

- 학습 강의 : Python Basic for AI 5-1 ~ 5-2강, AI Math 5 ~ 6강
- 과제 : 필수퀴즈 5, 6강, 선택과제 진행

## 1. 강의 복습
### **Python Basic for AI**
<br/>
**5-1강 : File/Exception/Log handling**
- 예상 가능한 예외, 예상 불가능한 예외가 존재
  - 예상이 불가능한 경우 Exception handling이 필요하다.
  - try ~ except 문법을 사용
- raise, assert 코드를 이용해 에러를 강제로 발생
- 파일은 텍스트 파일과 바이너리 파일로 나뉜다.
- file을 open할 때 w 모드와 a 모드의 차이점 : w는 새로 쓰는 것, a는 덧붙여서 쓰는 것(append 개념)
- os 모듈 사용법 : os.mkdir, os.path.join 등, pathlib 모듈도 있다.
- Pickle :  파이썬 객체를 영속화(persistence)하는 bulit-in 객체
- 로그 : 프로그램이 실행되면서 일어나는 정보를 기록한 것
- login level : debug, info, warning, error, critical
- 프로그램 실행할 때 설정하는 방법
  - configparser : 파일에 설정
  - argparser : 실핼 시점에 설정

<br/>
**5-2강 : Python data handling**
- 적자

### AI Math
5강 : 딥러닝 학습 방법 이해하기
- 비선형모델인 neural network의 원리
- softmax함수를 통해 선형모델을 확률벡터로 변환할 수 있다.
  - 분류 문제를 풀 때 선형모델과 softmax 함수를 결합하여 해결할 수 있다.
  - 추론의 경우 softmax함수가 아닌 one-hot 벡터를 이용
- 신경망은 선형모델과 활성함수를 합성한 함수이다. 이 때 활성함수는 하나의 실수값을 input으로 받는다.
  - 벡터나 행렬이 들어온다면 각각의 요소에 활성함수를 적용한다. 즉, 개별적으로 함수를 적용해 새로운 히든 벡터를 생성한다. 히든 벡터를 뉴런이라고 부른다. 
- 활성함수는 실수 범위에서 정의된 비선형 함수를 의미한다.
- 가중치 행렬과 활성함수를 여러 번 합성하게 되면 이것이 신경망이 되고 이를 순전파(forward propagation)라 한다.
- 역전파 알고리즘 : 경사하강법을 적용해 각각의 가중치 행렬들을 학습, 이 때 가중치 행렬들의 Gradient Vector를 계산해야 한다.
  - 연쇄법칙이 기반이 된다.  

6강 : 확률론 맛보기
- 데이터에서 관찰되는 분포와 모델 예측의 분포의 차이를 최소화하도록 학습한다. 즉, 확률론 기반으로 해석이 가능하다.
- 확률변수는 함수이다.
- 이산형 확률변수는 확률변수에 값을 가지는 모든 확률을 더해서 모델링하고 연속형 확률변수는 적분을 통해서 모델링한다. 
- 조건부 확률에 대한 설명
  - 데이터 x가 주어질 때 선형모델과 결합하여 softmax함수를 이용하면 확률벡터를 구할 수 있다.
- 기대값에 대한 설명
  - 이산형 확률 변수는 질량함수를 확률과 곱하여 급수를 취하고 연속형 확률 변수는 밀도함수 곱하여 적분을 취해 기대값을 구한다.
- 확률분포를 모를 때에는 몬테카를로 샘플링 방법을 사용 -> i.i.d(identical independent distributed)를 만족하는 확률변수여야 한다. 

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- 강의나 퀴즈는 Precourse에서 이미 한 번 했기 때문에 복습하는 차원에서 진행하였다.  

## 3. 참고할 만한 자료
- [Statistics110](https://www.boostcourse.org/ai152/joinLectures/195039){:target="_blank"}

## 4. 피어세션
- 필수퀴즈의 베이지안 공식에 대해 질문을 하고 답변을 받았다.
- 필수과제 4, 5에 대한 코드리뷰를 진행
- 자세한 내용은 [Week1_day3](https://github.com/round26/round26/wiki/Week-1-Day-3){:target="_blank"} 참조

## 5. 회고
- 다음부터는 아이패드 굿노트와 필기한 것을 활용하여 이미지도 첨부하여 글을 정리하면 좋을 것 같다.
- 오늘 멘토링을 진행했는데 스마트 팩토리, 스타트업에 대한 설명과 목적 의식, 동기 부여에 대한 좋은 조언을 얻었다. 그리고 가능하다면 캐글을 병행하는 게 많은 도움이 될 것이라고 하셨다. 그렇게 되게끔 만들어보자!

---
---

**Day 4**
===  
- 학습 강의 : Python Basic for AI 6강, AI Math 7 ~ 9강  
- 과제 : 선택과제1, 3

## 1. 강의 복습
### **Python Basic for AI**
<br/>
**6강 : numpy**
- numpy의 ndarray는 dynamic typing을 지원하지 않는다. -> 하나의 데이터만 가능!
- reshape를 사용할 때 -1을 통해 size를 자동적으로 계산한다.
- numpy에서 2차원 배열에 접근할 때 array[0, 1]과 같이 **,**로 접근할 수 있다.
- slicing 문법을 이용하여 짝수열만 출력하는 것이 가능하다. 
  - ex) ```arr[:, ::2]```
- something_like를 통해 기존 ndarray의 shape 크기 만큼 원하는 형태의 ndarray를 생성할 수 있다.
- operation functions을 이용할 때에는 axis 개념이 가장 중요하다. 새롭게 생긴 축이 axis = 0 이다. 
- numpy에서는 broadcasting에 주의하면서 연산을 해야한다.
- np.where에서 조건과 True일 때 값, False일 때의 값을 넣어주면 그 값이 적용된다.
```python
a = np.array([1,3,0])
np.where(a > 0) # index 반환
>>> array([0, 1])
np.where(a > 0, 3, 2) # True일 때 3, False일 때 2를 대입하여 반환
>>> array([3,3,2])
```

### **AI Math**
<br/>
**7강 : 통계론 맛보기**
- 통계적 모델링은 확률분포를 추정하는 것
- 모수적(paramatic) 방법론 : 데이터가 특정 확률분포를 따른다고 가정하고 그 분포의 모수를 추정하는 방법론
  - ex) 정규분포의 모수 $$\mu$$(평균), $$\sigma^2$$(분산)
- 데이터에 따라 모델의 구조와 모수의 개수가 변하면 비모수(nonparamatic) 방법론
- 기본적으로 확률분포에는 모수가 따라온다.(모수의 개수는 분포에 따라 다르다)
- **데이터 생성 방법에 따라 확률 분포를 가정해야 한다.** -> 데이터 관찰이 선행!
- 표본분포(sample distribution) ***VS*** 표집분포(sampling distribution)
  - 표집분포는 표본평군과 표본분산에 대한 확률분포
- 최대가능도 추정법(maximum likelihood estimation, MLE)  

  $$  \hat \theta_{MLE}=\underset{\theta}{\text{argmax}}\,L(\theta, x) = \underset{\theta}{\text{argmax}}\,P(x|\theta) $$   
     - 확률밀도함수 => $$\theta$$ (모수)가 주어졌을 때 $$x$$ 에 대한 함수  
     - 가능도함수 => 데이터 $$x$$ 에 대해서 $$\theta$$ (모수)를 변수로 가지는 함수 : 데이터가 주어진 상황
- 로그가능도를 사용해 곱셈 연산을 덧셈 연산으로 바꿔준다. -> 최적화 !!
- **기계학습의 원리 : 데이터로부터 확률분포 사이의 거리를 최소화**

<br/>
**8강 : 베이즈 통계학 맛보기**
- 데이터가 새로 추가될 때마다 정보를 업데이트 -> 베이즈 정리 !
- 조건부 확률로 인과 관계를 추론 X
- 데이터 분포의 변화에 강건한 예측모델 => **인과 관계 도입** but 정확도 down
- 중첩 요인을 제거하고 인과관계를 계산해야 한다.
  - Simpson's paradox
  - ex) 키와 지능 -> 키는 나이와 연관, 나이는 지능에 영향 -> 이러한 요인을 제거!!  

<br/>
**9강 : CNN 첫걸음**
- Convolution 연산 : 가중치 행렬이 아닌 커널(kernal)을 이용
  - 커널 : 고정된 가중치 행렬
  - 커널의 사이즈만큼만 입력 벡터에서 활용 -> 입력 벡터의 크기 - 커널의 크기 + 1
- 신호를 커널을 이용해 국소적으로 증폭 or 감소
- 다양한 차원에 적용 가능
- 채널이 여러 개인 2차원 입력의 경우 커널을 채널 개수에 맞춰 늘린 후 적용
- Convolution 연산의 역전파를 계산하게 되면 Convoultion 연산이 나온다.

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- numpy의 동작 원리와 여러 함수에 대해 새롭게 배웠다.
- 선택과제 1을 하면서 직접 손실 함수를 미분하여 Gradient를 계산하였고 경사 하강법의 원리에 대해 더 잘 파악할 수 있었다.
-  minibatch SGD은 주어진 데이터에서 minibatch 사이즈만큼 임의로 추출하여 모델을 학습시킨다.
- 선택과제 3을 통해 최대가능도 추정법에 대한 자세한 이론을 공부할 수 있었다.


## 3. 참고할 만한 자료
- [최대가능도 추정법](https://datascienceschool.net/02%20mathematics/09.02%20%EC%B5%9C%EB%8C%80%EA%B0%80%EB%8A%A5%EB%8F%84%20%EC%B6%94%EC%A0%95%EB%B2%95.html){:target="_blank"}
- [중심극한정리](https://ratsgo.github.io/statistics/2017/06/27/normal/){:target="_blank"}

## 4. 피어세션
- 선택과제1에 대해서 코드 리뷰를 진행
- 경사 하강법에서 벡터 연산에 대해 한 분의 Peer가 도움이 되는 내용을 공유
- 자세한 내용은 [Week1_day4](https://github.com/round26/round26/wiki/Week1-Day4){:target="_blank"} 참조

## 5. 회고
- 선택과제 3을 하면서 최대가능도 추정법에 대해 구글링을 했는데 선택과제 3과 동일한 글이 나왔다. ~~역시 구글이 짱인가?~~
- 오늘 임성빈 교수님의 마스터 클래스가 있었다. 기초 수학 선형대수학, 확률론, 통계학을 매우 강조하셨다. 추천해주신 사이트와 책을 통해 기초를 탄탄하게 다져야겠다. 가장 좋은 공부 방법은 정의를 여러 번 보고 어디에서 가장 많이 사용되는지 예제 위주로 공부하는 것이다. 
- 대학원 얘기도 많이 나왔는데 요약하자면 이미 많은 연구가 된 분야는 학사로도 충분하지만 새로운 분야라면 대학원이 훨씬 낫다. 아직 AI 공부 시작한 지 얼마 안 되었지만 공부하면서 관심가는 분야를 정하고 진로를 설정해야 할 것 같다.

---
---

**Day 5**
===  
- 학습 강의 : Python Basic for AI 7-1 ~ 7-2강, AI Math 10강  
- 과제 : 선택과제2

## 1. 강의 복습
### Python Basic for AI
<br/>
**7-1 ~ 7-2강 : pandas**
- series Object에 대한 설명
- loc은 index 이름으로 접근, iloc은 index position으로 접근 !
- 데이터 프레임의 작업할 때 본 프레임에 다시 할당해줘야 작업한 내용이 적용된다.
- numpy와 마찬가지로 fancy index, boolean index 사용 가능
- drop 함수는 axis 지정해서 축을 기준으로 제거(axis = 1 이라면 column이 삭제)
- 연산할 때는 항상 index가 기준이 된다.
- groupby 명령어로 데이터 프레임을 조작할 수 있다.
- Pivot Table, Crosstab 기능도 지원
- merge 할 때 join method(left, right, inner, outer)가 존재함

### **AI Math**
<br/>
**10강 : RNN 첫걸음**
- 시퀀스 데이터 : 순차적으로 들어오는 데이터(소리, 문자열, 주가 등)
  - i.i.d 가정을 위배하기 쉽다 (순서가 중요하기 때문)
- 과거의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부 확률을 사용
$$P(X_1,\cdots,X_n) = P(X_t|X_1,\cdots,X_{t-1})=\prod_{s=1}^tP(X_s|X_{s-1},\cdots,X_1)$$
- **과거 정보의 필요성에 따라 시퀀스 데이터의 모델링이 바뀐다.**

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- BPTT에 대해 공부하면서 역전파 알고리즘에 대해 더 자세하게 공부할 수 있었다.
- 경사 하강법과 역전파의 수식적 이해를 높였다.
- pandas 사용법에 많이 알 수 있었다.


## 3. 참고할 만한 자료
- [경사하강법](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931){:target="_blank"}
- [RNN](https://www.youtube.com/watch?v=RrB605Mbpic){:target="_blank"}
- [BPTT](https://m.blog.naver.com/infoefficien/221210061511){:target="_blank"}

## 4. 피어세션
- 서로 참고한 사이트 공유
- 필수과제 5에서 split()와 split(' ')의 차이점에 대해 논의
- 선택과제 2에 대해 많은 얘기를 했지만 해결하지 못했다. 주말을 이용해 공부 후 다시 논의
- 슬랙 채널을 활성화 하자!
- 자세한 내용은 [Week1_day5](https://github.com/round26/round26/wiki/Week1-Day5){:target="_blank"} 참조

## 5. 회고
- pandas에 대해 약간 헷갈리는 부분이 있지만 실습을 진행하면서 강의 영상을 참고하면 충분히 습득할 수 있을 것이라고 판단된다.
- RNN 부분과 확률론에서 어려움이 있었다. 주말과 부스트캠프 외적인 시간을 이용해서 추가적인 학습을 해야할 것 같다.




