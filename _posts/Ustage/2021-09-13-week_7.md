---
title:  "부스트캠프 AI Tech 7주차 주간 학습 정리"
excerpt: "Level 2 Ustage 7주차"
categories:
  - Ustage
tags:
  - [ustage]
toc: true
toc_sticky: false
date: 2021-09-13
author_profile: false
---
**Day 1**
===

- 학습 강의 : Part 4 - Transformer 7강
- 과제 : [선택과제 2] NMT training with Fairseq

## 1. 강의 복습
### **NLP** <br/>

**7강 : Transformer (1)**
- RNN은 Long-Term Dependency가 존재해서 이를 해결하기 위한 트릭으로 Bi-Directional RNNs 가 등장
![](../../assets/images/week7/week7_day1_bi.jpg)
    - 하지만 모델이 너무 무거워지고 학습하는데 많은 시간이 걸린다는 문제점이 존재
- Transformer model은 이러한 RNN model을 단점을 해결하기 위한 등장한 모델로 기존 Seq2seq with attention에서처럼 추가적인 module로 Attention을 활용하는 것이 아닌 Self-Attention만을 사용하여 학습을 할 수 있도록 구성했다.
![](../../assets/images/week7/week7_day1_transformer1.PNG)
    - 각 단어의 embedding vector가(I, go, home) Self-Attention을 거치면서 encoding되고 전체 문장의 sequence를 잘 반영한 output vector($h_1$, $h_2$, $h_3$)가 생성된다.
    - **$W^Q$ Metrix를 이용해 embedding vector를 선형변환한 것을 query vector라 한다.** 선형변환을 통해 query vector를 생성함으로써 다양한 가중치를 줄 수 있게 됐다. Seq2seq with attention에서 decoder의 hidden state vector 역할을 한다.
    - **$W^K$ Metrix를 이용해 embedding vector를 선형변환한 것을 key vector라 한다.** 이 벡터는 query vector와 내적연산을 통해 전체 sequence를 고려한 유사도를 구한다. Seq2seq with attention에서 encoder의 각 time step의 hidden states vector 역할을 한다.
    - 위에서 구한 유사도 vector를 softmax를 취해주면 가중치 vector가 된다.
    - **$W^V$ Metrix를 이용해 embedding vector를 선형변환한 것을 value vector라 한다.** 이 벡터를 가중치 벡터와 연산하여 선형 결합해주면 최종적인 encoder vector가 나온다.
    - 같은 embedding vector(I, go, home)이더라도 $W^Q$, $W^K$, $W^V$를 통해 Linear transformation을 진행해줌으로써 서로 다른 역할을 하는 벡터(Query, Key, Value)가 될 수 있다. 이러한 확장성 때문에 유연하게 유사도를 구하는 것이 가능해졌다.
    - key vector와 value vector 간의 관계를 살펴보자. query vector와 $k_1$이라는 key vector가 내적연산을 통해 유사도를 구하게 되면 이 유사도는 $v_1$이라는 value vector에 적용이 되어서 최종 결과가 나타난다. **즉 key vector의 개수와 value vector의 개수는 일치해야 한다는 것을 의미한다.**하지만 개수가 일치한다고 해서 둘의 차원이 같을 필요는 없다. value vector의 차원이 곧 output vector의 차원을 의미한다. value vector에 곱해지는 수치는 결국 key vector를 통해 구한 가중치 값(scalar)이기 때문이다.
    - 결론적으로 출력으로 나오는 output vector들은 전체 sequence의 각 단어 embedding vector에 대한 정보를 가중치에 따라 평균낸 vector이다.
    - 이는 sequence의 길이에 영향없이 sequence의 모든 정보를 취합할 수 있기 때문에 RNN의 Long-Term dependency를 근본적 해결했다고 볼 수 있다.
- 위 경우는 하나의 query vector에 따른 Self-Attention을 보여준 것이고 실제 행렬 연산 관점에서 바라보면 다음과 같다.
![](../../assets/images/week7/week7_day1_transformer2.jpg)
- 위 내용을 간단하게 요약하면 다음과 같다.
    1. Inputs은 Query vector $q$와 전제 Key-Value vectors 쌍이다.
    2. Output은 Value vector에 대한 가중 평균이다.
    3. 가중 평균에 쓰이는 가중치는 Query vector와 Value vector에 해당하는 Key vector의 내적이다.
    4. Query vector와 Key vector는 내적을 하기 때문에 차원이 같다.
    5. Value vector의 차원은 Output vector의 차원을 결정하고 Key vector 차원과는 무관하다.
    ![](../../assets/images/week7/week7_day1_attn.jpg)
- Query vector를 한 개에서 다수의 Query로 보고 행렬 연산 관점에서 보면 다음과 같다.
![](../../assets/images/week7/week7_day1_q.jpg)
    - 행렬 연산을 위해 Key Metrix를 Transpose한 것을 알 수 있다.
- 그림으로 표현하면 다음과 같다.
![](../../assets/images/week7/week7_day1_e.PNG)
    - 최종적으로는 **Scaled Dot-Product Attention**을 사용한다.
    - $A(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$
    - Query vector와 Key vector의 차원에 따라 분산이 결정되는데 이 분산이 크면 Softmax 함수를 취했을 때 어느 값에 치우쳐지는 경향이 존재한다.
    - 그렇기 때문에 $\sqrt{d_{k}}$로 나눠서 Query vector와 Key vector의 차원에 관계없이 분산을 1로 유지해준다.

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : 7_multi_head_attention**

- **선택과제 2 : NMT training with Fairseq**


## 3. 참고할 만한 자료
- **부스트코스 제공 자료**
  - [Attention is all you need, NeurIPS'17](https://arxiv.org/abs/1706.03762)
  - [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

- **슬랭 공유 자료 및 참고 자료**
  - [논문 코드 구현](https://github.com/bentrevett/pytorch-sentiment-analysis)

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/9-6-4bbb9980d82c461d9005ece5284fcc17) 참조

---
---

**Day 2**
===

- 학습 강의 : Part 4 - Transformer 8강
- 과제 : [선택과제 3] Byte Pair Encoding

## 1. 강의 복습
### **NLP** <br/>

**8강 : Transformer (2)**
- 동일한 $Q, K, V$ vector에 대해서 병렬적으로 동시에 여러 개의 Attention을 중첩한 것이 Multi-Head Attention이다.
![](../../assets/images/week7/week7_day2_m.PNG)
   - 각 Attention module마다 $W_Q, W_K, W_V$가 존재하고 각각 다른 파라미터 값을 가진다. 이러한 각각의 선형변환들을 Head라고 한다.
   - 각 Attention module마다 생성된 encoding vector를 concat하여 최종 encoding vector를 얻는다.
- Multi-Head Attention을 통해 동일한 sequence에서도 서로 다른 기준으로 다양한 정보를 가져올 수 있다. ex) 주체가 한 행동에 대한 정보를 가져오거나 혹은 주체가 있는 장소에 대한 정보를 가져온다.
![](../../assets/images/week7/week7_day2_mm.PNG)
  - 여러 개의 Head를 통해 여러 경우의 $Q, K, V$ vector를 얻고 이를 통해 각기 다른 정보를 가지는 encoding vector를 추출할 수 있다.
- 최종적으로 나온 encoding vector를 concat하고 다시 한 번 선형변환을 거쳐 최종 encoding vector를 생성한다.
![](../../assets/images/week7/week7_day2_mmm.PNG)
- Layer Type에 따른 연산량을 비교해보면 다음과 같다.
![](../../assets/images/week7/week7_day2_cal.PNG)
  - n은 문장의 길이(sequence length)
  - d는 hidden dimension
  - k는 cnn의 kernal size
  - r은 attention에서 전체 길이가 아닌 r개 만큼의 길이만 본다는 것을 의미한다.
  - Self-Attention의 경우 코어 수만 뒷받침된다면 병렬화를 통해 $O(1)$의 연산이 가능하다.
  - 반면 RNN의 경우 이전 time-step의 결과가 넘어와야 연산이 가능하기 때문에 병렬화가 불가능해서 $O(n)$의 연산이 가능하다.
  -  Self-Attention은 squence 길이에 제곱하는 메모리가 필요하기 때문에 RNN에 비해 연산속도는 빨라도 요구하는 메모리 요구량이 많다는 것을 알 수 있다.
- 실제 Transformer는 Multi-Head Attention에 Residual connection을 해주고 layer normalize를 진행한 후 나온 encoding vector를 다시 Fully connected layer를 거치고 Residual connection, layer normalize를 한 구조이다.
![](../../assets/images/week7/week7_day2_t.PNG)
  - Residual connection은 깊은 층을 쌓을 때 gradient vanish문제를 해결하여 학습을 안정화하는 역할을 한다.
  - Residual connection을 위해서는 입력 차원과 출력 차원이 같아야 한다.
  - Normalize layer는 주어진 값들에 대해 평균과 분산을 0, 1로 맞춰주고 그런 후 원하는 평균과 분산을 가지도록 하는 선형변환(Affine transformation)으로 구성된다.
  ![](../../assets/images/week7/week7_day2_n.PNG)
- 위에서 설명한 Transformer 구조에서는 문장의 sequence 정보를 반영할 수 없다. 이를 해결하기 위해 Positional Encoding을 적용한다. 
  - $P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{m o d e l}}\right)$
  $P E_{(p o s, 2 i+1)}=\cos \left(\right.$ pos $\left./ 10000^{2 i / d_{\text {model }}}\right)$
![](../../assets/images/week7/week7_day2_p.PNG)
    - 위 식을 토대로 input sequence 단어의 embedding vector에 단어의 위치에 해당하는 positional encoding vector를 더해줌으로써 위치 정보를 담는다.
- Transformer에서 사용된 Warm-up Learning Rate Scheduler
![](../../assets/images/week7/week7_day2_l.PNG)
  - optimizer에 사용되는 learning rate를 주어진 식에 따라 동적으로 바꿔준다.
  - 초반에는 iteration이 증가함에 따라 learning rate가 증가하고 어느 시점 이후부터는 감소하는 것을 확인할 수 있다.
- Transformer의 Encoder block을 통해 나온 encoding vector들이 어떤 정보를 주로 얻어오는지 시각화하면 다음과 같다.
![](../../assets/images/week7/week7_day2_w.PNG)
  - 한 단어가 sequence 내에서 특정 단어들의 정보를 많이 가져오는 것을 알 수 있다.
  - Head별로 추출하는 정보가 다른 것도 확인할 수 있다.
- Transformer의 Decoder를 살펴보면 다음과 같다.
![](../../assets/images/week7/week7_day2_l.jpg)
  - Encoder와 다른 점을 살펴보면 Decoder는 Masked Multi-Head Attention을 이용하는 것을 알 수 있다.
  - Encoder에서 나온 hidden state vector(encoding vector)들이 decoder의 두 번째 Multi-Head Attention의 Key, Value vector로써 입력으로 들어가고 Query vector는 Masked Multi-Head Attention를 통과한 Decoder의 encoding vector가 들어간다.
  - 그렇게 해서 나온 최종 encoding vector를 linear layer를 거쳐 output를 얻을 수 있다.
- 실제 inference 과정을 살펴보면 단어가 순차적으로 주어지기 때문에 처음 Timestep에서의 단어는 이후의 입력으로 주어지는 단어에 대한 정보를 얻으면 안된다. 이에 대한 문제를 해결하기 위한 것이 Masked Multi-Head Attention이다.
![](../../assets/images/week7/week7_day2_mask.PNG)
  - 실제 training 과정에서는 Decoder에 입력을 동시에 주지만 이는 실제 Inference 때에는 순차적으로 주기 때문에 뒤에 들어오는 단어에 대한 정보를 없애주어야 한다. 

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : 8_masked_multi_head_attention**
  
- **선택과제 3 : Byte Pair Encoding**

## 3. 참고할 만한 자료
- **부스트코스 제공 자료**
  - 

- **슬랭 공유 자료 및 참고 자료**
  - 

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/9-6-4bbb9980d82c461d9005ece5284fcc17) 참조