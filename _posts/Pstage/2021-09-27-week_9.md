---
title: "부스트캠프 AI Tech 9주차 주간 학습 정리"
excerpt: "Level 2 Pstage 9주차"
categories:
  - Pstage
tags:
  - [pstage]
toc: true
toc_sticky: false
date: 2021-09-27
author_profile: false
---

# **Day 1**

- 학습 강의 : KLUE 1 ~ 2강

## 1. 강의 복습

### **KLUE** <br/>

**1강 : 인공지능과 자연어처리**
- 자연어처리의 응용분야 
![](../../assets/images/week8/week8_day1_nlp_task.PNG)

- 인간의 자연어처리 : 화자는 떠올린 객체를 Text로 전달(인코딩) -> 청자는 Text를 객체로 전환(디코딩)
- 컴퓨터의 자연어처리 : Encoder가 벡터 형태로 자연어를 인코딩 -> Decoder는 벡터를 자연어로 디코딩

|인간의 자연어처리|컴퓨터의 자연어처리|
|:-:|:-:|
|![](../../assets/images/week8/week8_day1_h.PNG)|![](../../assets/images/week8/week8_day1_c.PNG)|
- 기존 자연어 임베딩 방식은 one-hot encoding 방식을 사용했다. But, 단어 벡터가 sparse해서 단어가 가지는 의미를 벡터 공간에 표현 불가능 -> Word2Vec 알고리즘 사용
![](../../assets/images/week8/week8_day1_o.PNG)
- Word2Vec 알고리즘 : 자연어의 의미를 벡터 공간에 임베딩, 주변 단어를 통해 단어의 의미 파악. But, 단어의 subword information을 무시하고 OOV에서 적용이 불가능하다. -> FastText 사용
- FastText : 단어를 n-gram으로 분리를 한 후, 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득, 오탈자, OOV, 등장 횟수가 적은 학습 단어에 대해 강세
![](../../assets/images/week8/week8_day1_f.PNG)
- 단어 임베딩 방식의 한계점 : 주변 단어를 통해 학습하기 때문에 문맥을 고려할 수 없다.
- 문맥을 고려하기 위해서 **언어모델**이 등장했다.
- 언어모델의 발전 과정
  1. Markov 기반의 언어모델 : Markov Chain Model로 다음의 단어나 문장이 나올 확률을 통게와 단어의 n-gram을 기반으로 계산
  2. 딥러닝을 활용한 RNN 기반의 언어모델 : 이전 state 정보가 다음 state를 예측하는데 사용
  3. 최종 context vector를 활용한 RNN 기반의 Seq2seq 언어모델 : Encoder layer에서 context vector를 획득하고 이를 Decoder layer에서 활용
      - 문제점 : 긴 문장의 경우 처음 token에 대한 정보가 희석, 하나의 context vector로 인한 병목 문제 -> Attention module
  4. Seq2seq with Attention 언어모델 : dynamic context vector 획득
      - 문제점 : RNN 기반이기 때문에 순차적으로 연산이 이뤄짐에 따라 연산 속도가 느림
  5. Self-attention 언어모델(Transformer)

<br/>

**2강 : 자연어의 전처리**
- 자연어 전처리 : raw data를 기계 학습 모델이 학습하는데 적합하게 만드는 프로세스
  - Task의 성능을 가장 확실하게 올릴 수 있는 방법!
- 자연어처리의 단계 : 
  - Task 설계
  - 필요 데이터 수집
  - 통계학적 분석 : Token 개수 -> 아웃라이어 제거, 빈도 확인 -> 사전 정의
  - 전처리 : 개행문자, 특수문자, 공백, 중복표현, 불용어, 조사 등등 제거
  - Tagging
  - Tokenizing : 자연어를 어떤 단위로 살펴볼 것인가(어절, 형태소)
  - 모델 설계, 구현
  - 성능 평가, 완료
- Python string 관련 함수 : 대소문자 변환 / 편집, 치환(strip, replace) / 분리, 결합(split, join) / 구성 문자열 판별(isdigit) / 검색(count) 
- 한국어 토큰화는 교착어이기 때문에 띄어쓰기 기준이 아니라 형태소 단위로 분리한다.

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : (2강) 자연어의 전처리 - 0_한국어전처리**
  - 파이썬의 정규표현식 사용법 : `r"<[^>]+>\s+(?=<)|<[^>]+>"`을 해석해보면 다음과 같다.
    - [^>]는 ">" 문자가 아닌 문자와 매칭이 된다. 
    - "(?+<)"의 경우는 긍정형 전방 탐색으로 (?=...) - ...에 해당되는 정규식과 매치되어야 하며 조건이 통과해도 문자열이 소비되지 않는다. 즉, 검색에는 포함되지만 검색 결과에는 제외된다.
    ```python
    >>> p = re.compile(".+(?=:)")
    >>> m = p.search("http://google.com")
    >>> print(m.group())
    http
    ```
  - 한국어 문장분리기 중, [kss 라이브러리](https://github.com/hyunwoongko/kss)를 많이 사용
  - 파이썬의 정규표현식 중 `\b, \B`가 있는데 이는 단어 구분자(Word boundary), 비단어 구분자를 의미한다. 자세한 내용은 [여기](https://wikidocs.net/4309)를 읽어보자
  - `re.compile(r"\((.*?)\)")`이런 형태로 `*?`을 결합하면 가능한 한 가장 최소한의 반복을 수행하도록 도와주는 역할을 한다.
- **실습코드 : (2강) 자연어의 전처리 - 1_한국어_토크나이징**
  - 한국어의 tokenizing 방식에는 어절 단위 : `split(" ")`, 형태소 단위 : `Mecab()`, 음절 단위 : `list(sentence)`, 자소 단위 : `hgtk`, WordPiece : `transformers` 등이 존재한다.
  - WordPiece tokenizing으로 `BertWordPieceTokenizer`를 사용할 수 있다.
## 3. 참고할 만한 자료

- **부스트코스 제공 자료**
  - [자연어처리](https://www.youtube.com/watch?v=jlCerj5eI4c)
  - [FastText](https://www.youtube.com/watch?v=7UA21vg4kKE)
  - [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
  - [Seq2Seq + attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)
- **슬랭 공유 자료 및 참고 자료**

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/9-13-12d1912a51dc46a48e6f9e8f009c57fc) 참조

---

---
