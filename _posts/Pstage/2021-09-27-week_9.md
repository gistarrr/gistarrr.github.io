---
title: "부스트캠프 AI Tech 9주차 주간 학습 정리"
excerpt: "Level 2 Pstage 9주차"
categories:
  - Pstage
tags:
  - [pstage]
toc: true
toc_sticky: false
date: 2021-09-27
author_profile: false
---

# **Day 1**

- 학습 강의 : KLUE 1 ~ 2강

## 1. 강의 복습

### **KLUE** <br/>

**1강 : 인공지능과 자연어처리**
- 자연어처리의 응용분야 
![](../../assets/images/week8/week8_day1_nlp_task.PNG)

- 인간의 자연어처리 : 화자는 떠올린 객체를 Text로 전달(인코딩) -> 청자는 Text를 객체로 전환(디코딩)
- 컴퓨터의 자연어처리 : Encoder가 벡터 형태로 자연어를 인코딩 -> Decoder는 벡터를 자연어로 디코딩

|인간의 자연어처리|컴퓨터의 자연어처리|
|:-:|:-:|
|![](../../assets/images/week8/week8_day1_h.PNG)|![](../../assets/images/week8/week8_day1_c.PNG)|
- 기존 자연어 임베딩 방식은 one-hot encoding 방식을 사용했다. But, 단어 벡터가 sparse해서 단어가 가지는 의미를 벡터 공간에 표현 불가능 -> Word2Vec 알고리즘 사용
![](../../assets/images/week8/week8_day1_o.PNG)
- Word2Vec 알고리즘 : 자연어의 의미를 벡터 공간에 임베딩, 주변 단어를 통해 단어의 의미 파악. But, 단어의 subword information을 무시하고 OOV에서 적용이 불가능하다. -> FastText 사용
- FastText : 단어를 n-gram으로 분리를 한 후, 모든 n-gram vector를 합산한 후 평균을 통해 단어 벡터를 획득, 오탈자, OOV, 등장 횟수가 적은 학습 단어에 대해 강세
![](../../assets/images/week8/week8_day1_f.PNG)
- 단어 임베딩 방식의 한계점 : 주변 단어를 통해 학습하기 때문에 문맥을 고려할 수 없다.
- 문맥을 고려하기 위해서 **언어모델**이 등장했다.
- 언어모델의 발전 과정
  1. Markov 기반의 언어모델 : Markov Chain Model로 다음의 단어나 문장이 나올 확률을 통게와 단어의 n-gram을 기반으로 계산
  2. 딥러닝을 활용한 RNN 기반의 언어모델 : 이전 state 정보가 다음 state를 예측하는데 사용
  3. 최종 context vector를 활용한 RNN 기반의 Seq2seq 언어모델 : Encoder layer에서 context vector를 획득하고 이를 Decoder layer에서 활용
      - 문제점 : 긴 문장의 경우 처음 token에 대한 정보가 희석, 하나의 context vector로 인한 병목 문제 -> Attention module
  4. Seq2seq with Attention 언어모델 : dynamic context vector 획득
      - 문제점 : RNN 기반이기 때문에 순차적으로 연산이 이뤄짐에 따라 연산 속도가 느림
  5. Self-attention 언어모델(Transformer)

<br/>

**2강 : 자연어의 전처리**
- 자연어 전처리 : raw data를 기계 학습 모델이 학습하는데 적합하게 만드는 프로세스
  - Task의 성능을 가장 확실하게 올릴 수 있는 방법!
- 자연어처리의 단계 : 
  - Task 설계
  - 필요 데이터 수집
  - 통계학적 분석 : Token 개수 -> 아웃라이어 제거, 빈도 확인 -> 사전 정의
  - 전처리 : 개행문자, 특수문자, 공백, 중복표현, 불용어, 조사 등등 제거
  - Tagging
  - Tokenizing : 자연어를 어떤 단위로 살펴볼 것인가(어절, 형태소)
  - 모델 설계, 구현
  - 성능 평가, 완료
- Python string 관련 함수 : 대소문자 변환 / 편집, 치환(strip, replace) / 분리, 결합(split, join) / 구성 문자열 판별(isdigit) / 검색(count) 
- 한국어 토큰화는 교착어이기 때문에 띄어쓰기 기준이 아니라 형태소 단위로 분리한다.

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : (2강) 자연어의 전처리 - 0_한국어전처리**
  - 파이썬의 정규표현식 사용법 : `r"<[^>]+>\s+(?=<)|<[^>]+>"`을 해석해보면 다음과 같다.
    - [^>]는 ">" 문자가 아닌 문자와 매칭이 된다. 
    - "(?+<)"의 경우는 긍정형 전방 탐색으로 (?=...) - ...에 해당되는 정규식과 매치되어야 하며 조건이 통과해도 문자열이 소비되지 않는다. 즉, 검색에는 포함되지만 검색 결과에는 제외된다.
    ```python
    >>> p = re.compile(".+(?=:)")
    >>> m = p.search("http://google.com")
    >>> print(m.group())
    http
    ```
  - 한국어 문장분리기 중, [kss 라이브러리](https://github.com/hyunwoongko/kss)를 많이 사용
  - 파이썬의 정규표현식 중 `\b, \B`가 있는데 이는 단어 구분자(Word boundary), 비단어 구분자를 의미한다. 자세한 내용은 [여기](https://wikidocs.net/4309)를 읽어보자
  - `re.compile(r"\((.*?)\)")`이런 형태로 `*?`을 결합하면 가능한 한 가장 최소한의 반복을 수행하도록 도와주는 역할을 한다.
- **실습코드 : (2강) 자연어의 전처리 - 1_한국어_토크나이징**
  - 한국어의 tokenizing 방식에는 어절 단위 : `split(" ")`, 형태소 단위 : `Mecab()`, 음절 단위 : `list(sentence)`, 자소 단위 : `hgtk`, WordPiece : `transformers` 등이 존재한다.
  - WordPiece tokenizing으로 `BertWordPieceTokenizer`가 존재하고 이 tokenizer에 vocab를 추가할 수 있다.
    ```python
    # Initialize an empty tokenizer
    tokenizer = BertWordPieceTokenizer(
      clean_text=True, # 토큰화 할 때 띄어쓰기 제거 
      handle_chinease_chars=True, # 한자의 경우는 음절 단위로
      strip_accents=False, # True: [YepHamza] -> [Yep, Hamza] 
      lowercase=False 
    )
    # Train
    tokenizer.train(
      files = "./data.txt",
      vocab_size=10000,
      min_frequency=2,
      show_progress=True,
      special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
      limit_alphabet=1000,
      wordpieces_prefix="##"
    )
    # Save
    tokenizer.save_model("Folder", "Name")
    ```
## 3. 참고할 만한 자료
- **Reference**
  - [서브워드 구축하기 ](https://keep-steady.tistory.com/37)
  - [Huggingface Transformers의 attention mask 문서](https://huggingface.co/transformers/glossary.html#attention-mask)
  - [Huggingface Transformers의 attention mask 구현](https://github.com/huggingface/tokenizers/tree/2fbd6779f6bdeb55c0fb9cceb3716ec20fc92646/bindings/python/py_src/tokenizers/implementations)
  - [한국어 전처리 위한 Huggingface + KoNLPy 실습](https://gist.github.com/lovit/259bc1d236d78a77f044d638d0df300c)
- **Further Reading**
  - [자연어처리](https://www.youtube.com/watch?v=jlCerj5eI4c)
  - [FastText](https://www.youtube.com/watch?v=7UA21vg4kKE)
  - [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
  - [Seq2Seq + attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)
  - [청와대 국민청원 데이터 전처리 (소개)](https://www.youtube.com/watch?v=9QW7QL8fvv0)
  - [청와대 국민청원 데이터 전처리 (실습)](https://www.youtube.com/watch?v=HIcXyyzefYQ)

## 4. 피어세션
- 강의 계획 설정
- Pstage 계획 설정
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/9-27-bef48c2ea53e4b8e8705548cd7e1bd31) 참조

---
---

# **Day 2**

- 학습 강의 : KLUE 3 ~ 4강

## 1. 강의 복습

### **KLUE** <br/>

**3강 : BERT 언어모델 소개**
![](../../assets/images/week8/week8_day2_a.PNG)
- Autoencoder에서 Encoder는 입력된 이미지를 압축된 형태로 표현하고(RNN에서의 context vector) Decoder는 원본 이미지를 복원하는 것이 목적이다. -> Compressed Data 정보를 가져오는 것은 입력된 이미지에 대한 벡터값이 된다.(자기 자신을 표현하도록 학습되었기 때문)
- BERT에서도 입력된 정보를 그대로 표현한다는 점에서 Autoencoder와 같은 맥락이지만 입력된 정보를 MASKING을 한다는 차이가 있다. -> 즉, masked된 자연어를 원본 자연어로 복원
- NLP 실험들에 대해 BERT 모델의 구조는 다음과 같다.
![](../../assets/images/week8/week8_day2_b.PNG)
  - 단일 문장 분류 : 감성 분석, 관계 추출
  - 두 문장 관계 분류 : 의미 비교
  - 문장 토큰 분류 : 개체명 분석
  - 기계 독해 정답 분류 : 기계 독해
- text를 토큰화할 때 형태소 단위로 분리한 후 Word Piece를 적용하는 것이 성능이 좋다.
- 관계 추출의 경우 Entity embedding layer를 추가하여 학습시켰을 때 성능이 훨씬 상승했다.
![](../../assets/images/week8/week8_day2_e.PNG)
<br/>

**4강 : 한국어 BERT 언어 모델 학습**
- BERT 학습의 단계 
  1. Tokenizer 만들기
  2. 데이터셋 확보
  3. Next Sentence Prediction(NSP)
  4. Masking
- 왜 새로 학습을 할까? : 도메인 특화 task의 경우, 도메인 특화 된 학습 데이터만 사용하는 것이 성능이 더 좋다!
![](../../assets/images/week8/week8_day2_d.PNG)
- 다음과 같은 과정을 통해 학습에 필요한 Dataset과 Dataloader를 구성해야 한다.
![](../../assets/images/week8/week8_day2_d1.PNG)
  - text를 tokenize해서 input_ids, token_type_ids형태로 구성하여 모델이 학습할 수 있는 Dataset 형태로 만들어 준다.
![](../../assets/images/week8/week8_day2_d2.PNG)
  - BERT 모델에 맞게 masking 처리를 한다.

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : (3강) BERT 언어 모델 소개 - 0_Huggingface**
  - 3줄의 코드로 pre-trained model과 tokenizer를 가져올 수 있다.
    ```python
    from transformers import AutoModel, AutoTokenizer
    model = TFAutoModel.from_pretrained("<model-name>")
    tokenizer = AutoTokenizer.from_pretrained("<model-name>")
    ```
  - `tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)`의 형식으로 tokenizer를 불러오고 tokenizer에 text를 태우면 input_ids, token_type_ids, attention_mask를 key값으로 가지는 encoding vector가 생성된다.(BertTokenizer의 경우)
  - `tokenizer.tokenize, tokenizer.encode, tokenizer.decode`등의 함수를 이용해서 순차적으로 토큰화가 어떻게 되는지 확인할 수 있다.
  - `tokenizer.tokenize()`에서 add_special_token, max_length, truncation, padding 등을 인자로 받을 수 있다.
  - `tokenizer.add_tokens()`를 통해 token을 추가해줄 수 있다. 
    - token을 추가해줄 경우 `model.resize_token_embeddings(len(tokenizer))`를 통해 embedding layer의 size를 변경해줘야 한다.
    - `tokenizer.vocab_size`는 token이 추가되기 전 vocab size가 출력된다.
  - tokenizer를 태운 값 : `input = tokenizer(text, return_tensors="pt")`이 model의 input으로 들어가면 출력값 : `outputs = model(**input)`이 나온다.
    - last_hidden_state는 `outputs.last_hidden_state`, [CLS] token에 대한 hidden state는 `outputs.pooler_output`으로 구할 수 있다.
  - `nn.CosineSimilarity(dim=1, eps=1e-6)`으로 문장의 유사도를 측정할 수 있다.
- **실습코드 : (3강) BERT 언어 모델 소개 - 1_BERT_유사도_기반_챗봇**
  - transformers 라이브러리에서 model을 받은 후 `model.paramters`를 하면 model의 구조를 확인할 수 있다.
  - bert pre-trained model의 outputs에서 last_hidden_state의 0번 째 index([CLS] token)를 pooler layer 통과시킨 결과가 pooler_output이다. pooler layer는 linear layer와 tanh가 결합된 layer이다.
  - `from sklearn.metrics.pairwise import cosine_similarity`을 이용해서 유사도를 계산할 수 있다.
- **실습코드 : (4강) 한국어 BERT 언어모델 학습 - 0_BERT_MASK_Attack**
  - `from transformers import pipeline`을 통해서 특정 task에 대해 쉽게 결과를 확인할 수 있다.
    ```python
    nlp_fill = pipeline('fill-mask', top_k=5, model=model, tokenizer=tokenizer)
    nlp_fill('Martin is living in [MASK].')
    >>> 
    [{'sequence': 'Martin is living in London.',
    'score': 0.04413441941142082,
    'token': 10829,
    'token_str': 'London'},
    {'sequence': 'Martin is living in Southampton.',
    'score': 0.016097432002425194,
    'token': 45113,
    'token_str': 'Southampton'},
    {'sequence': 'Martin is living in Italy.',
    'score': 0.01311422511935234,
    'token': 11619,
    'token_str': 'Italy'},
    {'sequence': 'Martin is living in a.',
    'score': 0.012310952879488468,
    'token': 169,
    'token_str': 'a'},
    {'sequence': 'Martin is living in Rome.',
    'score': 0.010854917578399181,
    'token': 14592,
    'token_str': 'Rome'}]
    ```


## 3. 참고할 만한 자료
- **Reference**
  - 
- **Further Reading**
  - 

## 4. 피어세션
- 4강 강의까지 질의 응답
- 멘토님에게 할 질문 리스트하기
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/9-28-ae4becdb086b4f2b996de28a0b12ff81) 참조

---
---