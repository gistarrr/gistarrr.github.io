---
title: "부스트캠프 AI Tech 11주차 주간 학습 정리"
excerpt: "Level 2 Pstage 11주차"
categories:
  - Pstage
tags:
  - [pstage]
toc: true
toc_sticky: false
date: 2021-10-13
author_profile: false
---

# **Day 1**
- 학습 강의 : MRC 1 ~ 2강

## 1. 강의 복습
### **MRC** <br/>

**1강 : MRC Intro & Python Basics**
- 기계 독해(Machine Reading Comprehension) : 주어진 지문(context)를 이해하고, 주어진 질의(Query/Question)의 답변을 추론하는 문제
    - Extractive Answer Datasets : 질의(Question)에 대한 답이 항상 주어진 지문(Context)의 segment(or span)으로 존재
        - Cloze Tests(CNN/Daily Mail, CBT)
        - Span Extraction(SQuAD, KorQuAD, NewsQA)
    - Descriptive/Narrative Answer Datasets : 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence(or free-form)의 형태 ex) MS MARCO, Narrative QA
    - Multiple-choice Datasets : 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태 ex) MCTest, RACE, ARC
- Challenges in MRC 
    - Paraphrasing : 같은 의미의 문장, but 다른 단어로 구성
    - Coreference Resolution
    - Unanswerable questions
    - Multi-hop reasoning : 여러 개의 문서에서 조합하여 질의에 대한 답을 도출
- MRC의 평가 방법
    - Exact Match & F1 score : Extractive Answer, Multiple-choice answer datasets
        - Exact Match or Accuracy : 에측한 답과 GT가 정확히 일치하는 샘플의 비율
        - F1 score : 에측한 답과 GT 사이의 token overlap을 F1으로 계산
        ![](../../assets/images/week11/week11_day1_m.PNG)
    - ROUGE-L / BLEU : Descriptive Answer Datasets
        - ROUGE-L score : 예측한 값과 GT 사이의 overlap recall(Longest Common Subsequence 기반, LCS)
        - BLEU : 예측한 답과 GT 사이의 precision
- **Unicode** : 전 세계의 모든 문자를 일관되게 표현하게 다룰 수 있도록 만들어진 문자셋, 각 문자마다 숫자 하나에 매핑
![](../../assets/images/week11/week11_day1_u.PNG)
- 인코딩 : 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것
- UTF-8(Unicode Transformation Format) : 문자 타입에 따라 다른 길이의 바이트를 할당
- **Python에서 Unicode 다루기**
    - ord : 문자를 유니코드로 변환, chr : 유니코드를 문자로 변환
- **토크나이징** : 텍스트를 토큰 단위로 나누는 것 / 단어(띄어쓰기 기준), 형태소, subword 등 여러 토큰 기준 사용
    - Subword 토크나이징 : 자주 쓰이는 글자 조합은 한 단위로 취급, 자주 쓰이지 않는 조합은 subword로 쪼갠다. --> "##"은 디코딩을 할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 의미
    ![](../../assets/images/week11/week11_day1_t.PNG)
    - tokenization 방법론은 모델의 일부이다.(해당 모델이 학습했던 방법과 똑같이 tokenize를 해주어야 하기 때문)
- BPE(Byte-Pair Encoding) : 데이터 압축용으로 제안된 알고리즘
    1. 가장 자주 나오는 글자 단위 Bigram(or Byte pair)를 다른 글자로 치환 
    2. 치환된 글자 저장 
    3. 위 과정 1 ~ 2번 반복
- HuggingFace datasets 라이브러리를 이용해 KorQuAD dataset을 빠르게 불러올 수 있다.
```python
from datasets import load_dataset
dataset = load_dataset('squad_kor_v1', split='train')
```

**2강 : Extraction-based MRC**
- Extraction-based MRC : 질문의 답변이 항상 주어진 지문 내에 span으로 존재 ex) SQuAD, KorQuAD, NewsQA, Natural Questions, etc.
- F1 score 계산 예시
![](../../assets/images/week11/week11_day1_f.PNG)
- Pre-processing
    - Tokenization : WordPiece Tokenizer 사용
    - Attention Mask, Token_type_ids --> pad는 0으로 처리
- 모델 출력값 : 연속된 단어토큰(span)에서 시작과 끝 위치를 예측
- Fine-tuning
![](../../assets/images/week11/week11_day1_fi.PNG)
    - 전체 토큰 중 가장 높은 값을 가지는 값이 출력된다.
- Post-processing
    - 불가능한 답 제거하기 : candidate list 제거
    - 최적의 답안 찾기

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : MRC Practice 1 - Looking into KorQuAD**
    - KorQuAD 데이터셋 살펴보기
    - `' '.join(tokenizer.tokenize(example_text)).replace(' ##', '')`로 역토크나이징 할 수 있다.(Bert 기반 Tokenizer)
    - tokenizer의 parameter 중 `return_overflowing_tokens=True`를 하면 context 문장이 짤리면 지정한 stride만큼 슬라이딩하여 뒤 문장을 tokenizer한다. --> tokenizing 한 후 key를 살펴보면 `'overflow_to_sample_mapping'`가 추가되어 있다.
    ![](../../assets/images/week11/week11_day1_e.PNG)
    - `return_offsets_mapping=True`를 이용하면 text를 tokenize했을 때 각 token에 대한 index가 원래 text 기준으로 나온다. 이를 이용하면 answer의 index를 tokenize했어도 찾을 수 있다.
    ![](../../assets/images/week11/week11_day1_o.PNG)
- **실습코드 : MRC Practice 2 - Extraction-based MRC**
    - HuggingFace의 datasets에서 load_metric을 통해 metric 함수를 불러올 수 있다.
    - max_seq_length를 정의하는 것이 중요하다.

## 3. 참고할 만한 자료
- **Further Reading**
    - [문자열 type에 관련된 정리글](https://kunststube.net/encoding/)
    - [KorQuAD 데이터 소개 슬라이드](https://www.slideshare.net/SeungyoungLim/korquad-introduction)
    - [Naver Engineering: KorQuAD 소개 및 MRC 연구 사례 영상](https://tv.naver.com/v/5564630)
    - [SQuAD 데이터셋 둘러보기](https://rajpurkar.github.io/SQuAD-explorer/)
    - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
    - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
    - [Huggingface datasets](https://huggingface.co/datasets)

## 4. 피어세션
- 강의 계획과 대회 계획 세우기
- 대회 관련 자료 공유
- 자세한 내용은 [Peer Session](https://diagnostic-offer-ddb.notion.site/10-12-38a08ed4616b46de852735be5f3902a4) 참조

---
---
# **Day 2**
- 학습 강의 : MRC 3 ~ 4강

## 1. 강의 복습
### **MRC** <br/>

**3강 : Generation-based MRC**
- MRC 문제를 푸는 방법
    - 1) Extraction-based mrc : 지문 내 답의 위치를 예측 --> 분류 문제
    - 2) Generation-based mrc : 주어진 지문과 질의를 보고, 답변을 생성 --> 생성 문제
- Generation-based MRC는 Seq2seq PLM 구조이다. (Extraction은 PLM + Classifier 구조)
- BART에서는 입력시퀀스에 대한 구분이 없기 때문에 token_type_ids가 존재하지 않는다.
- BART : 기계 독해, 기계 번역, 요약, 대화 등 Seq2seq 문제의 pre-training을 위한 denoising autoencoder
![](../../assets/images/week11/week11_day1_ba.PNG)
    - BART의 인코더는 BERT처럼 bi-directional, 디코더는 GPT처럼 uni-directional(autoregressive)
    ![](../../assets/images/week11/week11_day1_d.PNG)
    - BART는 텍스트에 노이즈를 주고 원래 텍스를 복구하는 문제로 Pre-training 되었다.
    ![](../../assets/images/week11/week11_day1_p.PNG)
- Generation-based mrc는 Decoder를 이용해 텍스트를 생성하기 때문에 이전 출력이 다음 스텝의 입력값으로 들어간다. 이 때 Decoding 방법론에는 Greedy Search, Exhaustive Search, Beam Search가 있다.

**4강 : Passage Retrieval - Sparse Embedding**
- 
<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : MRC Practice 3 - Generation-based MRC**
    - max_target_length를 지정해주어서 실제 타켓 텍스트의 길이에 제한을 둔다.
    
## 3. 참고할 만한 자료
- **Further Reading**
    - [Introducing BART](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html)
    - [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
    - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)](https://arxiv.org/abs/1910.10683)
## 4. 피어세션
- 
- 자세한 내용은 [Peer Session]() 참조

---
---

# **Day 3**
- 학습 강의 : MRC 5 ~ 6강

## 1. 강의 복습
### **MRC** <br/>

**5강 : MRC Intro & Python Basics**
- 

**6강 : Extraction-based MRC**
- 
<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : **
    - 

## 3. 참고할 만한 자료
- **Further Reading**
    - 

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session]() 참조

---
---
# **Day 4**
- 학습 강의 : MRC 7 ~ 8강

## 1. 강의 복습
### **MRC** <br/>

**7강 : MRC Intro & Python Basics**
- 

**8강 : Extraction-based MRC**
- 
<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : **
    - 

## 3. 참고할 만한 자료
- **Further Reading**
    - 

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session]() 참조

---
---

# **Day 5**
- 학습 강의 : MRC 9 ~ 10강

## 1. 강의 복습
### **MRC** <br/>

**9강 : MRC Intro & Python Basics**
- 

**10강 : Extraction-based MRC**
- 
<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : **
    - 

## 3. 참고할 만한 자료
- **Further Reading**
    - 

## 4. 피어세션
- 
- 자세한 내용은 [Peer Session]() 참조

---
---