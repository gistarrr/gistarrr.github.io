---
title: "부스트캠프 AI Tech 9주차 주간 학습 정리"
excerpt: "Level 2 Pstage 9주차"
categories:
  - Pstage
tags:
  - [pstage]
toc: true
toc_sticky: false
date: 2021-09-27
author_profile: false
---

# **Day 1**
- 학습 강의 : KLUE 1 ~ 2강

## 1. 강의 복습
### **MRC** <br/>

**1강 : MRC Intro & Python Basics**
- 기계 독해(Machine Reading Comprehension) : 주어진 지문(context)를 이해하고, 주어진 질의(Query/Question)의 답변을 추론하는 문제
    - Extractive Answer Datasets : 질의(Question)에 대한 답이 항상 주어진 지문(Context)의 segment(or span)으로 존재
        - Cloze Tests(CNN/Daily Mail, CBT)
        - Span Extraction(SQuAD, KorQuAD, NewsQA)
    - Descriptive/Narrative Answer Datasets : 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence(or free-form)의 형태 ex) MS MARCO, Narrative QA
    - Multiple-choice Datasets : 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태 ex) MCTest, RACE, ARC
- Challenges in MRC 
    - Paraphrasing : 같은 의미의 문장, but 다른 단어로 구성
    - Coreference Resolution
    - Unanswerable questions
    - Multi-hop reasoning : 여러 개의 문서에서 조합하여 질의에 대한 답을 도출
- MRC의 평가 방법
    - Exact Match & F1 score : Extractive Answer, Multiple-choice answer datasets
        - Exact Match or Accuracy : 에측한 답과 GT가 정확히 일치하는 샘플의 비율
        - F1 score : 에측한 답과 GT 사이의 token overlap을 F1으로 계산
        ![](../../assets/images/week11/week11_day1_m.PNG)
    - ROUGE-L / BLEU : Descriptive Answer Datasets
        - ROUGE-L score : 예측한 값과 GT 사이의 overlap recall(Longest Common Subsequence 기반, LCS)
        - BLEU : 예측한 답과 GT 사이의 precision
- **Unicode** : 전 세계의 모든 문자를 일관되게 표현하게 다룰 수 있도록 만들어진 문자셋, 각 문자마다 숫자 하나에 매핑
![](../../assets/images/week11/week11_day1_u.PNG)
- 인코딩 : 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것
- UTF-8(Unicode Transformation Format) : 문자 타입에 따라 다른 길이의 바이트를 할당
- **Python에서 Unicode 다루기**
    - ord : 문자를 유니코드로 변환, chr : 유니코드를 문자로 변환
- **토크나이징** : 텍스트를 토큰 단위로 나누는 것 / 단어(띄어쓰기 기준), 형태소, subword 등 여러 토큰 기준 사용
    - Subword 토크나이징 : 자주 쓰이는 글자 조합은 한 단위로 취급, 자주 쓰이지 않는 조합은 subword로 쪼갠다. --> "##"은 디코딩을 할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 의미
    ![](../../assets/images/week11/week11_day1_t.PNG)
    - tokenization 방법론은 모델의 일부이다.(해당 모델이 학습했던 방법과 똑같이 tokenize를 해주어야 하기 때문)
- BPE(Byte-Pair Encoding) : 데이터 압축용으로 제안된 알고리즘
    1. 가장 자주 나오는 글자 단위 Bigram(or Byte pair)를 다른 글자로 치환 
    2. 치환된 글자 저장 
    3. 위 과정 1 ~ 2번 반복
- HuggingFace datasets 라이브러리를 이용해 KorQuAD dataset을 빠르게 불러올 수 있다.
```python
from datasets import load_dataset
dataset = load_dataset('squad_kor_v1', split='train')
```

**2강 : Extraction-based MRC**
-

<br/>

## 2. 새로 알게된 내용 / 고민한 내용 (강의, 과제, 퀴즈)
- **실습코드 : MRC Practice 1 - Looking into KorQuAD**
    - KorQuAD 데이터셋 살펴보기
    - `' '.join(tokenizer.tokenize(example_text)).replace(' ##', '')`로 역토크나이징 할 수 있다.(Bert 기반 Tokenizer)
    - tokenizer의 parameter 중 `return_overflowing_tokens=True`를 하면 context 문장이 짤리면 지정한 stride만큼 슬라이딩하여 뒤 문장을 tokenizer한다. --> tokenizing 한 후 key를 살펴보면 `'overflow_to_sample_mapping'`가 추가되어 있다.
    ![](../../assets/images/week11/week11_day1_e.PNG)
    - `return_offsets_mapping=True`를 이용하면 text를 tokenize했을 때 각 token에 대한 index가 원래 text 기준으로 나온다. 이를 이용하면 answer의 index를 tokenize했어도 찾을 수 있다.
    ![](../../assets/images/week11/week11_day1_o.PNG)

## 3. 참고할 만한 자료
- **Further Reading**
    - [문자열 type에 관련된 정리글](https://kunststube.net/encoding/)
    - [KorQuAD 데이터 소개 슬라이드](https://www.slideshare.net/SeungyoungLim/korquad-introduction)
    - [Naver Engineering: KorQuAD 소개 및 MRC 연구 사례 영상](https://tv.naver.com/v/5564630)


## 4. 피어세션
- 
- 자세한 내용은 [Peer Session]() 참조

---
---
