---
title: "ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech 9ì£¼ì°¨ ì£¼ê°„ í•™ìŠµ ì •ë¦¬"
excerpt: "Level 2 Pstage 9ì£¼ì°¨"
categories:
  - Pstage
tags:
  - [pstage]
toc: true
toc_sticky: false
date: 2021-09-27
author_profile: false
---

# **Day 1**

- í•™ìŠµ ê°•ì˜ : KLUE 1 ~ 2ê°•

## 1. ê°•ì˜ ë³µìŠµ

### **KLUE** <br/>

**1ê°• : ì¸ê³µì§€ëŠ¥ê³¼ ìì—°ì–´ì²˜ë¦¬**
- ìì—°ì–´ì²˜ë¦¬ì˜ ì‘ìš©ë¶„ì•¼
  ![](../../assets/images/week8/week8_day1_nlp_task.PNG)

- ì¸ê°„ì˜ ìì—°ì–´ì²˜ë¦¬ : í™”ìëŠ” ë– ì˜¬ë¦° ê°ì²´ë¥¼ Textë¡œ ì „ë‹¬(ì¸ì½”ë”©) -> ì²­ìëŠ” Textë¥¼ ê°ì²´ë¡œ ì „í™˜(ë””ì½”ë”©)
- ì»´í“¨í„°ì˜ ìì—°ì–´ì²˜ë¦¬ : Encoderê°€ ë²¡í„° í˜•íƒœë¡œ ìì—°ì–´ë¥¼ ì¸ì½”ë”© -> DecoderëŠ” ë²¡í„°ë¥¼ ìì—°ì–´ë¡œ ë””ì½”ë”©

|                ì¸ê°„ì˜ ìì—°ì–´ì²˜ë¦¬                |               ì»´í“¨í„°ì˜ ìì—°ì–´ì²˜ë¦¬               |
| :---------------------------------------------: | :---------------------------------------------: |
| ![](../../assets/images/week8/week8_day1_h.PNG) | ![](../../assets/images/week8/week8_day1_c.PNG) |

- ê¸°ì¡´ ìì—°ì–´ ì„ë² ë”© ë°©ì‹ì€ one-hot encoding ë°©ì‹ì„ ì‚¬ìš©í–ˆë‹¤. But, ë‹¨ì–´ ë²¡í„°ê°€ sparseí•´ì„œ ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„ ë¶ˆê°€ëŠ¥ -> Word2Vec ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
  ![](../../assets/images/week8/week8_day1_o.PNG)
- Word2Vec ì•Œê³ ë¦¬ì¦˜ : ìì—°ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ì„ë² ë”©, ì£¼ë³€ ë‹¨ì–´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ì˜ë¯¸ íŒŒì•…. But, ë‹¨ì–´ì˜ subword informationì„ ë¬´ì‹œí•˜ê³  OOVì—ì„œ ì ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. -> FastText ì‚¬ìš©
- FastText : ë‹¨ì–´ë¥¼ n-gramìœ¼ë¡œ ë¶„ë¦¬ë¥¼ í•œ í›„, ëª¨ë“  n-gram vectorë¥¼ í•©ì‚°í•œ í›„ í‰ê· ì„ í†µí•´ ë‹¨ì–´ ë²¡í„°ë¥¼ íšë“, ì˜¤íƒˆì, OOV, ë“±ì¥ íšŸìˆ˜ê°€ ì ì€ í•™ìŠµ ë‹¨ì–´ì— ëŒ€í•´ ê°•ì„¸
  ![](../../assets/images/week8/week8_day1_f.PNG)
- ë‹¨ì–´ ì„ë² ë”© ë°©ì‹ì˜ í•œê³„ì  : ì£¼ë³€ ë‹¨ì–´ë¥¼ í†µí•´ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ë¬¸ë§¥ì„ ê³ ë ¤í•  ìˆ˜ ì—†ë‹¤.
- ë¬¸ë§¥ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ì„œ **ì–¸ì–´ëª¨ë¸**ì´ ë“±ì¥í–ˆë‹¤.
- ì–¸ì–´ëª¨ë¸ì˜ ë°œì „ ê³¼ì •
  1. Markov ê¸°ë°˜ì˜ ì–¸ì–´ëª¨ë¸ : Markov Chain Modelë¡œ ë‹¤ìŒì˜ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì´ ë‚˜ì˜¬ í™•ë¥ ì„ í†µê²Œì™€ ë‹¨ì–´ì˜ n-gramì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
  2. ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ RNN ê¸°ë°˜ì˜ ì–¸ì–´ëª¨ë¸ : ì´ì „ state ì •ë³´ê°€ ë‹¤ìŒ stateë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©
  3. ìµœì¢… context vectorë¥¼ í™œìš©í•œ RNN ê¸°ë°˜ì˜ Seq2seq ì–¸ì–´ëª¨ë¸ : Encoder layerì—ì„œ context vectorë¥¼ íšë“í•˜ê³  ì´ë¥¼ Decoder layerì—ì„œ í™œìš©
     - ë¬¸ì œì  : ê¸´ ë¬¸ì¥ì˜ ê²½ìš° ì²˜ìŒ tokenì— ëŒ€í•œ ì •ë³´ê°€ í¬ì„, í•˜ë‚˜ì˜ context vectorë¡œ ì¸í•œ ë³‘ëª© ë¬¸ì œ -> Attention module
  4. Seq2seq with Attention ì–¸ì–´ëª¨ë¸ : dynamic context vector íšë“
     - ë¬¸ì œì  : RNN ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— ìˆœì°¨ì ìœ¼ë¡œ ì—°ì‚°ì´ ì´ë¤„ì§ì— ë”°ë¼ ì—°ì‚° ì†ë„ê°€ ëŠë¦¼
  5. Self-attention ì–¸ì–´ëª¨ë¸(Transformer)

<br/>

**2ê°• : ìì—°ì–´ì˜ ì „ì²˜ë¦¬**

- ìì—°ì–´ ì „ì²˜ë¦¬ : raw dataë¥¼ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ”ë° ì í•©í•˜ê²Œ ë§Œë“œëŠ” í”„ë¡œì„¸ìŠ¤
  - Taskì˜ ì„±ëŠ¥ì„ ê°€ì¥ í™•ì‹¤í•˜ê²Œ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•!
- ìì—°ì–´ì²˜ë¦¬ì˜ ë‹¨ê³„ :
  - Task ì„¤ê³„
  - í•„ìš” ë°ì´í„° ìˆ˜ì§‘
  - í†µê³„í•™ì  ë¶„ì„ : Token ê°œìˆ˜ -> ì•„ì›ƒë¼ì´ì–´ ì œê±°, ë¹ˆë„ í™•ì¸ -> ì‚¬ì „ ì •ì˜
  - ì „ì²˜ë¦¬ : ê°œí–‰ë¬¸ì, íŠ¹ìˆ˜ë¬¸ì, ê³µë°±, ì¤‘ë³µí‘œí˜„, ë¶ˆìš©ì–´, ì¡°ì‚¬ ë“±ë“± ì œê±°
  - Tagging
  - Tokenizing : ìì—°ì–´ë¥¼ ì–´ë–¤ ë‹¨ìœ„ë¡œ ì‚´í´ë³¼ ê²ƒì¸ê°€(ì–´ì ˆ, í˜•íƒœì†Œ)
  - ëª¨ë¸ ì„¤ê³„, êµ¬í˜„
  - ì„±ëŠ¥ í‰ê°€, ì™„ë£Œ
- Python string ê´€ë ¨ í•¨ìˆ˜ : ëŒ€ì†Œë¬¸ì ë³€í™˜ / í¸ì§‘, ì¹˜í™˜(strip, replace) / ë¶„ë¦¬, ê²°í•©(split, join) / êµ¬ì„± ë¬¸ìì—´ íŒë³„(isdigit) / ê²€ìƒ‰(count)
- í•œêµ­ì–´ í† í°í™”ëŠ” êµì°©ì–´ì´ê¸° ë•Œë¬¸ì— ë„ì–´ì“°ê¸° ê¸°ì¤€ì´ ì•„ë‹ˆë¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œë‹¤.

<br/>

## 2. ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš© / ê³ ë¯¼í•œ ë‚´ìš© (ê°•ì˜, ê³¼ì œ, í€´ì¦ˆ)

- **ì‹¤ìŠµì½”ë“œ : (2á„€á…¡á†¼) á„Œá…¡á„‹á…§á†«á„‹á…¥á„‹á…´ á„Œá…¥á†«á„á…¥á„…á…µ - 0\_á„’á…¡á†«á„€á…®á†¨á„‹á…¥á„Œá…¥á†«á„á…¥á„…á…µ**
  - íŒŒì´ì¬ì˜ ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ë²• : `r"<[^>]+>\s+(?=<)|<[^>]+>"`ì„ í•´ì„í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
    - [^>]ëŠ” ">" ë¬¸ìê°€ ì•„ë‹Œ ë¬¸ìì™€ ë§¤ì¹­ì´ ëœë‹¤.
    - "(?+<)"ì˜ ê²½ìš°ëŠ” ê¸ì •í˜• ì „ë°© íƒìƒ‰ìœ¼ë¡œ (?=...) - ...ì— í•´ë‹¹ë˜ëŠ” ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ì–´ì•¼ í•˜ë©° ì¡°ê±´ì´ í†µê³¼í•´ë„ ë¬¸ìì—´ì´ ì†Œë¹„ë˜ì§€ ì•ŠëŠ”ë‹¤. ì¦‰, ê²€ìƒ‰ì—ëŠ” í¬í•¨ë˜ì§€ë§Œ ê²€ìƒ‰ ê²°ê³¼ì—ëŠ” ì œì™¸ëœë‹¤.
    ```python
    >>> p = re.compile(".+(?=:)")
    >>> m = p.search("http://google.com")
    >>> print(m.group())
    http
    ```
  - í•œêµ­ì–´ ë¬¸ì¥ë¶„ë¦¬ê¸° ì¤‘, [kss ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/hyunwoongko/kss)ë¥¼ ë§ì´ ì‚¬ìš©
  - íŒŒì´ì¬ì˜ ì •ê·œí‘œí˜„ì‹ ì¤‘ `\b, \B`ê°€ ìˆëŠ”ë° ì´ëŠ” ë‹¨ì–´ êµ¬ë¶„ì(Word boundary), ë¹„ë‹¨ì–´ êµ¬ë¶„ìë¥¼ ì˜ë¯¸í•œë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://wikidocs.net/4309)ë¥¼ ì½ì–´ë³´ì
  - `re.compile(r"\((.*?)\)")`ì´ëŸ° í˜•íƒœë¡œ `*?`ì„ ê²°í•©í•˜ë©´ ê°€ëŠ¥í•œ í•œ ê°€ì¥ ìµœì†Œí•œì˜ ë°˜ë³µì„ ìˆ˜í–‰í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (2ê°•) ìì—°ì–´ì˜ ì „ì²˜ë¦¬ - 1*í•œêµ­ì–´*í† í¬ë‚˜ì´ì§•**
  - í•œêµ­ì–´ì˜ tokenizing ë°©ì‹ì—ëŠ” ì–´ì ˆ ë‹¨ìœ„ : `split(" ")`, í˜•íƒœì†Œ ë‹¨ìœ„ : `Mecab()`, ìŒì ˆ ë‹¨ìœ„ : `list(sentence)`, ìì†Œ ë‹¨ìœ„ : `hgtk`, WordPiece : `transformers` ë“±ì´ ì¡´ì¬í•œë‹¤.
  - WordPiece tokenizingìœ¼ë¡œ `BertWordPieceTokenizer`ê°€ ì¡´ì¬í•˜ê³  ì´ tokenizerì— vocabë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.
    ```python
    # Initialize an empty tokenizer
    tokenizer = BertWordPieceTokenizer(
      clean_text=True, # í† í°í™” í•  ë•Œ ë„ì–´ì“°ê¸° ì œê±°
      handle_chinease_chars=True, # í•œìì˜ ê²½ìš°ëŠ” ìŒì ˆ ë‹¨ìœ„ë¡œ
      strip_accents=False, # True: [YepHamza] -> [Yep, Hamza]
      lowercase=False
    )
    # Train
    tokenizer.train(
      files = "./data.txt",
      vocab_size=10000,
      min_frequency=2,
      show_progress=True,
      special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"],
      limit_alphabet=1000,
      wordpieces_prefix="##"
    )
    # Save
    tokenizer.save_model("Folder", "Name")
    ```

## 3. ì°¸ê³ í•  ë§Œí•œ ìë£Œ

- **Reference**
  - [ì„œë¸Œì›Œë“œ êµ¬ì¶•í•˜ê¸° ](https://keep-steady.tistory.com/37)
  - [Huggingface Transformersì˜ attention mask ë¬¸ì„œ](https://huggingface.co/transformers/glossary.html#attention-mask)
  - [Huggingface Transformersì˜ attention mask êµ¬í˜„](https://github.com/huggingface/tokenizers/tree/2fbd6779f6bdeb55c0fb9cceb3716ec20fc92646/bindings/python/py_src/tokenizers/implementations)
  - [í•œêµ­ì–´ ì „ì²˜ë¦¬ ìœ„í•œ Huggingface + KoNLPy ì‹¤ìŠµ](https://gist.github.com/lovit/259bc1d236d78a77f044d638d0df300c)
- **Further Reading**
  - [ìì—°ì–´ì²˜ë¦¬](https://www.youtube.com/watch?v=jlCerj5eI4c)
  - [FastText](https://www.youtube.com/watch?v=7UA21vg4kKE)
  - [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
  - [Seq2Seq + attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)
  - [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì†Œê°œ)](https://www.youtube.com/watch?v=9QW7QL8fvv0)
  - [ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„° ì „ì²˜ë¦¬ (ì‹¤ìŠµ)](https://www.youtube.com/watch?v=HIcXyyzefYQ)

## 4. í”¼ì–´ì„¸ì…˜
- ê°•ì˜ ê³„íš ì„¤ì •
- Pstage ê³„íš ì„¤ì •
- ìì„¸í•œ ë‚´ìš©ì€ [Peer Session](https://diagnostic-offer-ddb.notion.site/9-27-bef48c2ea53e4b8e8705548cd7e1bd31) ì°¸ì¡°

---
---

# **Day 2**
- í•™ìŠµ ê°•ì˜ : KLUE 3 ~ 4ê°•

## 1. ê°•ì˜ ë³µìŠµ
### **KLUE** <br/>

**3ê°• : BERT ì–¸ì–´ëª¨ë¸ ì†Œê°œ**
![](../../assets/images/week8/week8_day2_a.PNG)
- Autoencoderì—ì„œ EncoderëŠ” ì…ë ¥ëœ ì´ë¯¸ì§€ë¥¼ ì••ì¶•ëœ í˜•íƒœë¡œ í‘œí˜„í•˜ê³ (RNNì—ì„œì˜ context vector) DecoderëŠ” ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. -> Compressed Data ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ì…ë ¥ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ë²¡í„°ê°’ì´ ëœë‹¤.(ìê¸° ìì‹ ì„ í‘œí˜„í•˜ë„ë¡ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸)
- BERTì—ì„œë„ ì…ë ¥ëœ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í‘œí˜„í•œë‹¤ëŠ” ì ì—ì„œ Autoencoderì™€ ê°™ì€ ë§¥ë½ì´ì§€ë§Œ ì…ë ¥ëœ ì •ë³´ë¥¼ MASKINGì„ í•œë‹¤ëŠ” ì°¨ì´ê°€ ìˆë‹¤. -> ì¦‰, maskedëœ ìì—°ì–´ë¥¼ ì›ë³¸ ìì—°ì–´ë¡œ ë³µì›
- NLP ì‹¤í—˜ë“¤ì— ëŒ€í•´ BERT ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
  ![](../../assets/images/week8/week8_day2_b.PNG)
  - ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ : ê°ì„± ë¶„ì„, ê´€ê³„ ì¶”ì¶œ
  - ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ : ì˜ë¯¸ ë¹„êµ
  - ë¬¸ì¥ í† í° ë¶„ë¥˜ : ê°œì²´ëª… ë¶„ì„
  - ê¸°ê³„ ë…í•´ ì •ë‹µ ë¶„ë¥˜ : ê¸°ê³„ ë…í•´
- textë¥¼ í† í°í™”í•  ë•Œ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œ í›„ Word Pieceë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ì¢‹ë‹¤.
- ê´€ê³„ ì¶”ì¶œì˜ ê²½ìš° Entity embedding layerë¥¼ ì¶”ê°€í•˜ì—¬ í•™ìŠµì‹œì¼°ì„ ë•Œ ì„±ëŠ¥ì´ í›¨ì”¬ ìƒìŠ¹í–ˆë‹¤.
  ![](../../assets/images/week8/week8_day2_e.PNG)
  <br/>

**4ê°• : í•œêµ­ì–´ BERT ì–¸ì–´ ëª¨ë¸ í•™ìŠµ**
- BERT í•™ìŠµì˜ ë‹¨ê³„
  1. Tokenizer ë§Œë“¤ê¸°
  2. ë°ì´í„°ì…‹ í™•ë³´
  3. Next Sentence Prediction(NSP)
  4. Masking
- ì™œ ìƒˆë¡œ í•™ìŠµì„ í• ê¹Œ? : ë„ë©”ì¸ íŠ¹í™” taskì˜ ê²½ìš°, ë„ë©”ì¸ íŠ¹í™” ëœ í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤!  
  ![](../../assets/images/week8/week8_day2_d.PNG)
- ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í†µí•´ í•™ìŠµì— í•„ìš”í•œ Datasetê³¼ Dataloaderë¥¼ êµ¬ì„±í•´ì•¼ í•œë‹¤.
  - textë¥¼ tokenizeí•´ì„œ input_ids, token_type_idsí˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” Dataset í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤.
    ![](../../assets/images/week8/week8_day2_d1.PNG)
  - BERT ëª¨ë¸ì— ë§ê²Œ masking ì²˜ë¦¬ë¥¼ í•œë‹¤.
    ![](../../assets/images/week8/week8_day2_d2.PNG)
    <br/>

## 2. ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš© / ê³ ë¯¼í•œ ë‚´ìš© (ê°•ì˜, ê³¼ì œ, í€´ì¦ˆ)
- **ì‹¤ìŠµì½”ë“œ : (3ê°•) BERT ì–¸ì–´ ëª¨ë¸ ì†Œê°œ - 0_Huggingface**
  - 3ì¤„ì˜ ì½”ë“œë¡œ pre-trained modelê³¼ tokenizerë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.
    ```python
    from transformers import AutoModel, AutoTokenizer
    model = TFAutoModel.from_pretrained("<model-name>")
    tokenizer = AutoTokenizer.from_pretrained("<model-name>")
    ```
  - `tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)`ì˜ í˜•ì‹ìœ¼ë¡œ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¤ê³  tokenizerì— textë¥¼ íƒœìš°ë©´ input_ids, token_type_ids, attention_maskë¥¼ keyê°’ìœ¼ë¡œ ê°€ì§€ëŠ” encoding vectorê°€ ìƒì„±ëœë‹¤.(BertTokenizerì˜ ê²½ìš°)
  - `tokenizer.tokenize, tokenizer.encode, tokenizer.decode`ë“±ì˜ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ìˆœì°¨ì ìœ¼ë¡œ í† í°í™”ê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
  - `tokenizer.tokenize()`ì—ì„œ add_special_token, max_length, truncation, padding ë“±ì„ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆë‹¤.
  - `tokenizer.add_tokens()`ë¥¼ í†µí•´ tokenì„ ì¶”ê°€í•´ì¤„ ìˆ˜ ìˆë‹¤.
    - tokenì„ ì¶”ê°€í•´ì¤„ ê²½ìš° `model.resize_token_embeddings(len(tokenizer))`ë¥¼ í†µí•´ embedding layerì˜ sizeë¥¼ ë³€ê²½í•´ì¤˜ì•¼ í•œë‹¤.
    - `tokenizer.vocab_size`ëŠ” tokenì´ ì¶”ê°€ë˜ê¸° ì „ vocab sizeê°€ ì¶œë ¥ëœë‹¤.
  - tokenizerë¥¼ íƒœìš´ ê°’ : `input = tokenizer(text, return_tensors="pt")`ì´ modelì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ì¶œë ¥ê°’ : `outputs = model(**input)`ì´ ë‚˜ì˜¨ë‹¤.
    - last_hidden_stateëŠ” `outputs.last_hidden_state`, [CLS] tokenì— ëŒ€í•œ hidden stateëŠ” `outputs.pooler_output`ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.
  - `nn.CosineSimilarity(dim=1, eps=1e-6)`ìœ¼ë¡œ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (3ê°•) BERT ì–¸ì–´ ëª¨ë¸ ì†Œê°œ - 1*BERT*ìœ ì‚¬ë„*ê¸°ë°˜*ì±—ë´‡**
  - transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ modelì„ ë°›ì€ í›„ `model.paramters`ë¥¼ í•˜ë©´ modelì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
  - bert pre-trained modelì˜ outputsì—ì„œ last_hidden_stateì˜ 0ë²ˆ ì§¸ index([CLS] token)ë¥¼ pooler layer í†µê³¼ì‹œí‚¨ ê²°ê³¼ê°€ pooler_outputì´ë‹¤. pooler layerëŠ” linear layerì™€ tanhê°€ ê²°í•©ëœ layerì´ë‹¤.
  - `from sklearn.metrics.pairwise import cosine_similarity`ì„ ì´ìš©í•´ì„œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (4ê°•) í•œêµ­ì–´ BERT ì–¸ì–´ëª¨ë¸ í•™ìŠµ - 0_BERT_MASK_Attack**
  - `from transformers import pipeline`ì„ í†µí•´ì„œ íŠ¹ì • taskì— ëŒ€í•´ ì‰½ê²Œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
    ```python
    nlp_fill = pipeline('fill-mask', top_k=5, model=model, tokenizer=tokenizer)
    nlp_fill('Martin is living in [MASK].')
    >>>
    [{'sequence': 'Martin is living in London.',
    'score': 0.04413441941142082,
    'token': 10829,
    'token_str': 'London'},
    {'sequence': 'Martin is living in Southampton.',
    'score': 0.016097432002425194,
    'token': 45113,
    'token_str': 'Southampton'},
    {'sequence': 'Martin is living in Italy.',
    'score': 0.01311422511935234,
    'token': 11619,
    'token_str': 'Italy'},
    {'sequence': 'Martin is living in a.',
    'score': 0.012310952879488468,
    'token': 169,
    'token_str': 'a'},
    {'sequence': 'Martin is living in Rome.',
    'score': 0.010854917578399181,
    'token': 14592,
    'token_str': 'Rome'}]
    ```
- **ì‹¤ìŠµì½”ë“œ : (4ê°•) í•œêµ­ì–´ BERT ì–¸ì–´ëª¨ë¸ í•™ìŠµ - 1\_í•œêµ­ì–´\_BERT_pre_training**
  - `from tokenizers import BertWordPieceTokenizer`ì„ í†µí•´ í•™ìŠµì‹œí‚¬ tokenizerë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
  - BertWordPieceTokenizerì˜ parameterë¡œëŠ” clean_text(ë„ì–´ì“°ê¸° ê°™ì€ ê³µë°± ì œê±°), handle_chinese_chars(í•œìëŠ” char ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°), strip_accents, lowercase ë“±ì´ ìˆë‹¤.
  - ìœ„ì™€ ê°™ì´ ì„¤ì •í•œ tokenizerë¥¼ files, vocab_size, min_frequency, special_tokens, wordpieces_prefix ë“±ì˜ ì¸ìë¥¼ í†µí•´ í•™ìŠµ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
  - Pretrainì„ í•˜ê¸° ìœ„í•´ì„œëŠ” NSPë¡œ ì´ë£¨ì–´ì§„ Dataset, Masking ëœ Datasetì´ í•„ìš”í•˜ë‹¤. Masking datasetì˜ ê²½ìš° Transformersì—ì„œ DataCollatorForLanguageModelingì´ë€ í•¨ìˆ˜ë¥¼ ì œê³µí•´ì¤€ë‹¤.
  - pipelineì„ ì‚¬ìš©í•˜ì—¬ Pretrain modelì„ í™•ì¸í•  ë•Œ cpuì¸ì§€, gpuì¸ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.(gpuì¼ ê²½ìš°, pipelineì˜ ì¸ìë¡œ `device=0`)
  - ìš”ì•½í•´ë³´ë©´ Tokenizerë¥¼ í•™ìŠµì‹œì¼œ ë‚˜ë§Œì˜ Tokenizerë¥¼ ë§Œë“¤ê³  ì´ì— ë”°ë¼ BertConfigë¥¼ ì„¤ì •í•˜ê³  `BertForPreTraining(config=config)`ë¡œ Bert ê»ë°ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤. ê·¸ëŸ° í›„ Maskingê³¼ NSPì— ë§ëŠ” Datasetë¥¼ êµ¬ì„±í•˜ê³  Trainerì˜ ì—¬ëŸ¬ argë¥¼ ì„¤ì •í•´ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„ ê°’ì„ ì €ì¥í•œë‹¤(config.json, pytorch_model.bin). ê·¸ëŸ° ë‹¤ìŒ `BertForMaskedLM.from_pretrained('model_output')`ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‹¤.

## 3. ì°¸ê³ í•  ë§Œí•œ ìë£Œ

- **Reference**
  - Paper: ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805)
  - [BERT í†ºì•„ë³´ê¸°](https://docs.likejazz.com/bert/)
  - [LM training from scratch](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw)
  - ë‚˜ë§Œì˜ BERT Wordpiece Vocab ë§Œë“¤ê¸°
    - [Wordpiece Vocab ë§Œë“¤ê¸°](https://monologg.kr/2020/04/27/wordpiece-vocab/)
    - [Wordpiece Tokenizer ë§Œë“¤ê¸°](https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0) -[Extracting training data from large language model](https://www.youtube.com/watch?v=NGoDUEz3tZg)
  - [BERT ì¶”ê°€ ì„¤ëª…](https://jiho-ml.com/weekly-nlp-28/)

## 4. í”¼ì–´ì„¸ì…˜

- 4ê°• ê°•ì˜ê¹Œì§€ ì§ˆì˜ ì‘ë‹µ
- ë©˜í† ë‹˜ì—ê²Œ í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸í•˜ê¸°
- ìì„¸í•œ ë‚´ìš©ì€ [Peer Session](https://diagnostic-offer-ddb.notion.site/9-28-ae4becdb086b4f2b996de28a0b12ff81) ì°¸ì¡°

---
---

# **Day 3**

- í•™ìŠµ ê°•ì˜ : KLUE 5 ~ 6ê°•

## 1. ê°•ì˜ ë³µìŠµ

### **KLUE** <br/>

**5ê°• : BERT ê¸°ë°˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ**
- KLUE : í•œêµ­ì–´ ìì—°ì–´ ì´í•´ ë°´ì¹˜ë§ˆí¬
  - ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ : ë¬¸ì¥ ë¶„ë¥˜, ê´€ê³„ ì¶”ì¶œ
  - ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ì˜ ìœ ì‚¬ë„([CLS]) : ë¬¸ì¥ ìœ ì‚¬ë„
  - ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ : ìì—°ì–´ ì¶”ë¡ 
  - ë¬¸ì¥ í† í° ë¶„ë¥˜ : ê°œì²´ëª… ì¸ì‹, í’ˆì‚¬ íƒœê¹…, ì§ˆì˜ ì‘ë‹µ
  - ëª©ì í˜• ëŒ€í™”, ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„
- ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„ : ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” task
  - ì˜ì¡´ì†Œì™€ ì§€ë°°ì†Œë¥¼ ì´ìš©í•´ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶„ì„
  - Sequence labeling ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.  
  ![](../../assets/images/week8/week8_day2_t.PNG)
  - **ë³µì¡í•œ ìì—°ì–´ í˜•íƒœë¥¼ ê·¸ë˜í”„ë¡œ êµ¬ì¡°í™”í•´ì„œ í‘œí˜„ì´ ê°€ëŠ¥! ê° ëŒ€ìƒì— ëŒ€í•œ ì •ë³´ ì¶”ì¶œì´ ê°€ëŠ¥**
- ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ taskë€ ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì–´ë–¤ ì¢…ë¥˜ì˜ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” task
  - ê°ì •ë¶„ì„(Sentiment Analysis), ì£¼ì œ ë¼ë²¨ë§(Topic Labeling), ì–¸ì–´ê°ì§€(Language Detection), ì˜ë„ ë¶„ë¥˜(Intent Classification)
  - Kor_hate, Kor_sarcasm, Kor_sae, Kor_3i4k ë“±ì˜ ë¬¸ì¥ ë¶„ë¥˜ ë°ì´í„°ê°€ ì¡´ì¬
- ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ì—ì„œ ëª¨ë¸ êµ¬ì¡°ë„ë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
![](../../assets/images/week8/week8_day2_m.PNG)
  - Bertì˜ [CLS] tokenì˜ vectorë¥¼ classification í•˜ëŠ” Dense Layerì— ì‚¬ìš©í•œë‹¤.
  - ì£¼ìš” ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
    - input_ids : sequence tokenì„ ì…ë ¥
    - attention_mask : [0,1]ë¡œ íŒ¨ë”© í† í° êµ¬ë¶„
    - token_type_ids : [0,1]ë¡œ ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ë¬¸ì¥ êµ¬ë¶„
    - position_ids : ê° ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì„ë² ë”© ì¸ë±ìŠ¤
    - inputs_embeds : input_ids ëŒ€ì‹  ì§ì ‘ ì„ë² ë”© í‘œí˜„ì„ í• ë‹¹
    - labels : loss ê³„ì‚°ì„ ìœ„í•œ ë ˆì´ë¸”
    - Next_sentence_label : ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ loss ê³„ì‚°ì„ ìœ„í•œ ë ˆì´ë¸”
- í•™ìŠµ ê³¼ì •ì„ ë‹¤ìŒê³¼ ê°™ë‹¤.
![](../../assets/images/week8/week8_day2_p.PNG)

<br/>

**6ê°• : BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ**
- ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ taskë€ ì£¼ì–´ì§„ 2ê°œì˜ ë¬¸ì¥ì— ëŒ€í•´, ë‘ ë¬¸ì¥ì˜ ìì—°ì–´ ì¶”ë¡ ê³¼ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” task
![](../../assets/images/week9/week9_day1_t.PNG)
  - Bertì˜ [CLS] token ìœ„ì— Headë¥¼ ë¶€ì°©í•˜ì—¬ ë¶„ë¥˜ë¥¼ í•œë‹¤.
- Natural Language Inference(NLI)
  - ì–¸ì–´ëª¨ë¸ì´ ìì—°ì–´ì˜ ë§¥ë½ì„ ì´í•´í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” task
  - ì „ì œë¬¸ì¥(Premise)ê³¼ ê°€ì„¤ë¬¸ì¥(Hypothesis)ì„ Entailment(í•¨ì˜), Contradiction(ëª¨ìˆœ), Neutral(ì¤‘ë¦½)ìœ¼ë¡œ ë¶„ë¥˜
  ![](../../assets/images/week9/week9_day1_n.PNG)
- Semantic text pair
  - ë‘ ë¬¸ì¥ì˜ ì˜ë¯¸ê°€ ì„œë¡œ ê°™ì€ ë¬¸ì¥ì¸ì§€ ê²€ì¦í•˜ëŠ” task
- Information Retrieval Question and Answering(IRQA)
  - ì±—ë´‡ì„ ìœ„í•œ taskë¡œì¨ ì‚¬ì „ì— ì •ì˜í•´ë†“ì€ QA setì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ëŠ” task
<br/>

## 2. ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš© / ê³ ë¯¼í•œ ë‚´ìš© (ê°•ì˜, ê³¼ì œ, í€´ì¦ˆ)
- **ì‹¤ìŠµì½”ë“œ : (5ê°•) BERT ê¸°ë°˜ ë‹¨ì¼ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 0_ë‹¨ì¼ë¬¸ì¥ë¶„ë¥˜**
  - HuggingFaceì˜ datasetsë¥¼ ì´ìš©í•˜ë©´ ë‹¤ì–‘í•œ datasetì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
  - `datasets.load_datasets('nsmc')`ë¡œ nsmc(ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë°ì´í„°)ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.
  - ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì…‹ì€ train, testë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆë‹¤.
  - pandas í•¨ìˆ˜ ì¤‘ drop_duplicatesë¥¼ ì´ìš©í•˜ë©´ ì¤‘ë³µê°’ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤. dropnaë¥¼ ì´ìš©í•˜ë©´ nullê°’ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤.
  - ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œ BERT model ìœ„ì— classificationì„ ìœ„í•œ Headë¥¼ ë¶€ì°©í•´ì•¼ í•˜ëŠ”ë° transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆë‹¤. --> BertForSequenceClassification 
  - HuggingFaceì˜ Trainerë¥¼ ì‚¬ìš©í•  ë•Œ, compute_metrics í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë©´ evaluateë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ í•  ìˆ˜ ìˆë‹¤.
  ```python
  trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics)
  trainer.evaluate(eval_dataset=test_dataset)
  ```
- **ì‹¤ìŠµì½”ë“œ : (6ê°•) BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 0_í•™ìŠµ_ë°ì´í„°_êµ¬ì¶•**
  -  ë‹¨ìˆœíˆ ë‹¤ë¥¸ ë¬¸ì¥ì„ ëœë¤í•˜ê²Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ í•™ìŠµì˜ ë‚œì´ë„ê°€ ë§¤ìš° ì‰½ë‹¤. --> keyë¡œ ì„¤ì •ëœ ë¬¸ì¥ê³¼ ê´€ë ¨ì—†ëŠ” ë¬¸ì¥ë“¤ì„ embeddingí•˜ê³  cosine similarityê°€ ë†’ì€ ë¬¸ì¥ì„ ì„ íƒí•´ì„œ ì´ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. 
  - ë¬¸ì¥ì˜ embedding vectorëŠ” ìœ ì‚¬í•˜ì§€ë§Œ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë‹¤ë¥¸ ë¬¸ì¥ì´ê¸° ë•Œë¬¸ì— í•™ìŠµ ë‚œì´ë„ê°€ ì˜¬ë¼ê°„ë‹¤. 
  - keywordëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì„ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (6ê°•) BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 1_ë‘_ë¬¸ì¥_ê´€ê³„_ë¶„ë¥˜_í•™ìŠµ**
  - ìœ„ ì‹¤ìŠµì„ í†µí•´ êµ¬ì„±í•œ datasetì˜ êµ¬ì¡°ëŠ” `sentence1 \t sentence2 \t label \n`ì´ê³  ì´ë¥¼ train, testë¡œ ë‚˜ëˆ„ì–´ ì¤€ë‹¤.
  - pandasì˜ groupby().ngroupsë¥¼ í•˜ë©´ groupì˜ ê°œìˆ˜ê°€ ë‚˜ì˜¨ë‹¤.
  - tokenizerì— ë¬¸ì¥ 2ê°œë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´ ìë™ìœ¼ë¡œ [CLS], [SEP] tokenì„ ë¶™ì—¬ì£¼ê³  token_type_idsë¥¼ ìƒì„±í•´ì¤€ë‹¤.
  ```python
  tokenized_train_sentences = tokenizer(
    list(train_data['sent_a'][0:]),
    list(train_data['sent_b'][0:]),
    return_tensors="pt",
    padding=True,
    truncation=True,
    add_special_tokens=True,
    max_length=64
    )
  ```
- **ì‹¤ìŠµì½”ë“œ : (6ê°•) BERT ê¸°ë°˜ ë‘ ë¬¸ì¥ ê´€ê³„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 2_BERT_IRQA_ì±—ë´‡**
  - ì²« ë²ˆì§¸ ëª¨ë¸ì„ ì´ìš©í•´ ì…ë ¥ëœ ë¬¸ì¥ê³¼ ë‹¤ë¥¸ ë¬¸ì¥ì˜ [CLS] tokenì˜ ìœ ì‚¬ë„ë¥¼ í†µí•´ Top-nê°œì˜ QA setì„ ë½‘ëŠ”ë‹¤.
  - ì´ ë½‘íŒ ë°ì´í„°ë¥¼ Paraphrase Detectioní•˜ëŠ” ë‘ ë²ˆì§¸ ëª¨ë¸ì— íƒœì›Œ ì‹¤ì œ Queryì™€ QA setì˜ Questionì´ ìœ ì‚¬í•œì§€ í™•ì¸í•˜ê³  ìœ ì‚¬í•˜ë‹¤ë©´ QA setì˜ Answerë¥¼ ê²°ê³¼ë¡œ ì¶œë ¥í•œë‹¤.
  - ì±—ë´‡ì˜ ê²½ìš°, ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¤ìŠµê³¼ ê°™ì´ ì‚¬ì „ì •ì˜(QA set)ë¥¼ í•´ë†“ê³  IRQA ëª¨ë¸ì„ íƒœì›Œ ë‹µë³€ì„ ì¶œë ¥í•˜ê³  ë‹µë³€ì„ ì¶œë ¥í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì˜ ê²½ìš° ìƒì„± ëª¨ë¸ì„ íƒœì›Œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì±—ë´‡ì„ êµ¬ì„±í•œë‹¤.
## 3. ì°¸ê³ í•  ë§Œí•œ ìë£Œ
- **Reference**
  - classification
    - [BERT Text Classification Using Pytorch](https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b)
    - [Sentiment Analysis with BERT](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)
    - [ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì •ë¶„ì„](https://colab.research.google.com/drive/1tIf0Ugdqg4qT7gcxia3tL7und64Rv1dP)
    - [Sequence Classification using Pytorch Lightning with BERT](https://knswamy.medium.com/sequence-classification-using-pytorch-lightning-with-bert-on-imbd-data-5e9f48baa638)
    - [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
## 4. í”¼ì–´ì„¸ì…˜
- íŒ€ì›ë“¤ ê°„ module ë³„ë¡œ í•  ì¼ì„ ë‚˜ëˆ„ì–´ì„œ ëŒ€íšŒ ì§„í–‰
- ìì„¸í•œ ë‚´ìš©ì€ [Peer Session](https://diagnostic-offer-ddb.notion.site/9-29-3adcd89d83d4455c826660e9206abd11) ì°¸ì¡°

---
---

# **Day 4**

- í•™ìŠµ ê°•ì˜ : KLUE 7 ~ 8ê°•

## 1. ê°•ì˜ ë³µìŠµ

### **KLUE** <br/>

**7ê°• : BERT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ë¬¸ì¥ í† í° ë¶„ë¥˜**
- ë¬¸ì¥ í† í° ê´€ê³„ ë¶„ë¥˜ taskë€ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° tokenì´ ì–´ë–¤ ë²”ì£¼ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” task
![](../../assets/images/week9/week9_day1_r.PNG)
  - ê° tokenë§ˆë‹¤ classification layerê°€ ë¶€ì°©ë˜ì–´ tokenì„ ë¶„ë¥˜í•œë‹¤.
- Named Entity Recognition(NER)
  - ê°œì²´ëª… ì¸ì‹ì€ ë¬¸ë§¥ì„ íŒŒì•…í•´ì„œ ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“±ê³¼ ê°™ì€ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì„œì—ì„œ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì–´ ë˜ëŠ” ì–´êµ¬(ê°œì²´) ë“±ì„ ì¸ì‹í•˜ëŠ” task
  - ê°™ì€ ë‹¨ì–´ë¼ë„ ë¬¸ë§¥ì—ì„œ ë‹¤ì–‘í•œ ê°œì²´(Entity)ë¡œ ì‚¬ìš©ëœë‹¤.
- Part-of-speech tagging(POS TAGGING)
  - ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ê° ì„±ë¶„ì— ëŒ€í•˜ì—¬ ê°€ì¥ ì•Œë§ëŠ” í’ˆì‚¬ë¥¼ íƒœê¹…í•˜ëŠ” task
- kor_nerì´ë¼ëŠ” NER taskì— ë§ëŠ” datasetì´ ì¡´ì¬(pos taggingë„ í•¨ê»˜ ì¡´ì¬)
![](../../assets/images/week9/week9_day1_k.PNG)
  - Entity tagì—ì„œ BIO tagë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. BëŠ” ê°œì²´ëª…ì˜ ì‹œì‘(Begin), IëŠ” ë‚´ë¶€(Inside), OëŠ” ë‹¤ë£¨ì§€ ì•ŠëŠ” ê°œì²´ëª…(Outside)ë¥¼ ì˜ë¯¸ 
  - B-PERì€ ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ì‹œì‘, I-PERëŠ” ì¸ë¬¼ëª… ê°œì²´ëª…ì˜ ë‚´ë¶€ ë¶€ë¶„
- ë¬¸ì¥ í† í° ê´€ê³„ ë¶„ë¥˜ taskì˜ ê²½ìš° WordPiece tokenizerë¥¼ ì´ìš©í•˜ê²Œ ë˜ë©´ ë‹¨ì–´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìë¥´ì§€ ëª»í•´ ê°œì²´ëª…ì´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆë‹¤. --> ìŒì ˆ ë‹¨ìœ„ë¡œ tokenize í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. 
<br/>

**8ê°• : í•œêµ­ì–´ BERT ì–¸ì–´ ëª¨ë¸ í•™ìŠµ**
- GPT ì–¸ì–´ ëª¨ë¸ì€ ìì—°ì–´ ìƒì„±ì— íŠ¹í™”ëœ ëª¨ë¸ì´ë‹¤.
![](../../assets/images/week9/week9_day4_g.PNG)
  - GPTì—ì„œëŠ” ì§€ë„ í•™ìŠµì„ í•„ìš”ë¡œ í•˜ê³  labeled dataê°€ í•„ìˆ˜ì ì´ë¼ëŠ” ë‹¨ì ì´ ì¡´ì¬, íŠ¹ì • taskë¥¼ ìœ„í•´ fine-tuningëœ ëª¨ë¸ì€ ë‹¤ë¥¸ taskì—ì„œ ì‚¬ìš© ë¶ˆê°€ëŠ¥ --> GPT-2ì˜ ë°°ê²½ : fine-tuningì´ í•„ìš”í•˜ì§€ ì•ŠëŠ” ëª¨ë¸
- **GPT-2** : ì´ë¯¸ ì—„ì²­ë‚œ ë°ì´í„°ë¡œ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— êµ³ì´ fine-tuning í•˜ì§€ ì•Šì•„ë„ ëœë‹¤! --> Zero-shot, One-shot, Few-shot
![](../../assets/images/week9/week9_day4_2.PNG)
- GPT decoderì™€ GPT-2 decoder
![](../../assets/images/week9/week9_day4_d.PNG)
  - ê±°ì˜ ë¹„ìŠ·í•œ êµ¬ì¡°ì§€ë§Œ ë” ë§ì´ ìŒ“ì•„ì„œ GPT-2ëŠ” ë” ë§ì€ parameterë¥¼ ì‚¬ìš©
- **GPT-3**ëŠ” GPT-2ë³´ë‹¤ ë” ë§ì€ parameterì™€ ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ Pretrain ì‹œì¼°ë‹¤.
![](../../assets/images/week9/week9_day4_3.PNG)
  - Transformerì˜ Decoder êµ¬ì¡°ë¥¼ í™œìš©
- GPT ì–¸ì–´ ëª¨ë¸ì€ Weight updateê°€ ì—†ê¸° ë•Œë¬¸ì— ìƒˆë¡œìš´ ì§€ì‹ í•™ìŠµì´ ì—†ë‹¤ë¼ëŠ” ë‹¨ì ì´ ì¡´ì¬ --> ì‹œê¸°ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ë¬¸ì œì— ëŒ€ì‘ ë¶ˆê°€
- í…ìŠ¤íŠ¸ ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼ ë©€í‹° ëª¨ë‹¬ì— ì •ë³´ë„ í•„ìš”

<br/>

## 2. ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš© / ê³ ë¯¼í•œ ë‚´ìš© (ê°•ì˜, ê³¼ì œ, í€´ì¦ˆ)
- **ì‹¤ìŠµì½”ë“œ : (7á„€á…¡á†¼) BERT á„€á…µá„‡á…¡á†« á„†á…®á†«á„Œá…¡á†¼ á„á…©á„á…³á†« á„‡á…®á†«á„…á…² á„†á…©á„ƒá…¦á†¯ á„’á…¡á†¨á„‰á…³á†¸ - 0_á„†á…®á†«á„Œá…¡á†¼_á„á…©á„á…³á†«_á„ƒá…¡á†«á„‹á…±_á„’á…¡á†¨á„‰á…³á†¸ + (w_KLUE)**
  - pathlibì˜ Path.read_text()ëŠ” íŒŒì¼ í•˜ë‚˜ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì½ì–´ì˜¨ë‹¤.
  - tagë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œëŠ” tagë˜í•œ idë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
  - CLS, SEP ë“± íŠ¹ìˆ˜ í† í°ë„ token labelì— ë“±ë¡ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— 'O'ë¡œ ë“±ë¡í•´ì¤€ë‹¤.  
  - HuggingFaceì˜ BertForTokenClassification Classë¥¼ í˜¸ì¶œí•¨ìœ¼ë¡œì¨ ë¬¸ì¥ í† í° ë¶„ë¥˜ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤. (`model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags))`)
  - ìŒì ˆ ë‹¨ìœ„ tokenizerë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— Inference ê³¼ì •ì—ì„œë„ ìŒì ˆ ë‹¨ìœ„ tokenizerë¥¼ ê±°ì¹œ í›„ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•œë‹¤.
  - KLUE ë°ì´í„°ì˜ ê²½ìš° ì´ë¯¸ ìŒì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (7ê°•) BERT ê¸°ë°˜ ë¬¸ì¥ í† í° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ - 1_ê¸°ê³„_ë…í•´_í•™ìŠµ + (w_KLUE)**
  - `tokenizer.char_to_token(batch_index, char_index)` : batch_indexì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì—ì„œ char_indexì— í•´ë‹¹ë˜ëŠ” wordë¥¼ tokenizerë¡œ tokenize í–ˆì„ ë•Œ í•´ë‹¹ wordê°€ ìœ„ì¹˜í•˜ëŠ” token indexë¡œ ë³€í™˜í•´ì¤€ë‹¤.
  - ì •ë‹µì´ 512 token ë’¤ì— ì¡´ì¬í•  ê²½ìš° Bert Modelì— ë“¤ì–´ê°€ëŠ” max_seq_length ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— ì—ëŸ¬ ì²˜ë¦¬ë¥¼ í•´ì¤˜ì•¼ í•œë‹¤.
  - HuggingFaceì˜ BertForQuestionAnswering Classë¥¼ ì´ìš©í•´ ê¸°ê³„ë…í•´ taskë¥¼ ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
- **ì‹¤ìŠµì½”ë“œ : (8ê°•) GPT ì–¸ì–´ ëª¨ë¸ ì†Œê°œ - 0_í•œêµ­ì–´_GPT_2_pre_training**
  - GPT-2ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ SentencePieceBPETokenizerë¥¼ ì´ìš©í•œë‹¤.
  - GPT-2ëŠ” special tokenìœ¼ë¡œ \<s>(ë¬¸ì¥ì˜ ì‹œì‘), \<pad>, \</s>(ë¬¸ì¥ì˜ ë), \<unk> ë“±ì´ ìˆë‹¤.
  - Model Configì— ìì‹ ì´ ë§Œë“  tokenizerì˜ vocab_sizeë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤.
  ```python
  config = GPT2Config(
  vocab_size=tokenizer.get_vocab_size(),
  bos_token_id=tokenizer.token_to_id("<s>"),
  eos_token_id=tokenizer.token_to_id("</s>"),
  )
  ```
  - GPT-2 ëª¨ë¸ì€ BERT ëª¨ë¸ê³¼ ë‹¬ë¦¬ì…ë ¥ ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ tokenì—ì„œ ë‹¤ìŒ tokenë¥¼ ì˜ˆì¸¡í•˜ëŠ” í˜•íƒœë¡œ í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì—  Masking, NSPê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤. 
## 3. ì°¸ê³ í•  ë§Œí•œ ìë£Œ
- **Reference**
  - ê°œì²´ëª…ì¸ì‹
    - [1. Named Entity Recognition (NER) for Turkish with BERT](https://medium.com/analytics-vidhya/named-entity-recognition-for-turkish-with-bert-f8ec04a31b0)
  - QA
    - [1. lonformer_qa_training.ipynb](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
    - [2. [ë…¼ë¬¸ë¦¬ë·°] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://jeonsworld.github.io/NLP/rag/)
  - BERT seq2seq
    - [1. BERT2BERT_for_CNN_Dailymail.ipynb](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)
    - [2. Bert2Bert Summarization](https://github.com/MrBananaHuman/bert2bert-summarization)
  - GPT
    - [1. Paper : Language Understanding](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  - GPT-2
    - [1. Paper : Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - GPT-3
    - [1. Paper : Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  - ì–¸ì–´ëª¨ë¸ì˜ í•™ìŠµë²•
    - [1. Week 19 - ì–¸ì–´ ëª¨ë¸ì„ ê°€ì§€ê³  íŠ¸ëŸ¼í”„ ë´‡ ë§Œë“¤ê¸°?!](https://jiho-ml.com/weekly-nlp-19/)

## 4. í”¼ì–´ì„¸ì…˜
- ëŒ€íšŒ ì‹œë„ ê²°ê³¼ ê³µìœ 
- ìì„¸í•œ ë‚´ìš©ì€ [Peer Session](https://diagnostic-offer-ddb.notion.site/9-30-fe7bc175039d4147b3842a89f46f6683) ì°¸ì¡°

---
---

# **Day 5**

- í•™ìŠµ ê°•ì˜ : KLUE 9 ~ 10ê°•

## 1. ê°•ì˜ ë³µìŠµ

### **KLUE** <br/>

**9ê°• : GPT ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ì˜ ìì—°ì–´ ìƒì„±**
- 
<br/>

**10ê°• : ìµœì‹  ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬**
- XLNet : Relative positional encoding ë°©ì‹ ì ìš©, Permutation language modeling ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
- RoBERTa : Model í•™ìŠµ ì‹œê°„ ì¦ê°€ + Batch size ì¦ê°€ + Train data ì¦ê°€, NSP ì œê±°, Longer sentence ì¶”ê°€, **Dynamic masking**(ë˜‘ê°™ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ maskingì„ 10 ë²ˆ ë‹¤ë¥´ê²Œ ì ìš©í•˜ì—¬ í•™ìŠµ)
- BART : Transformer Encoder-Decoder í†µí•© LM
![](../../assets/images/week9/week9_day5_b.PNG)
  - ì–´ë ¤ìš´ taskë¥¼ ëª¨ë‘ ì ìš©ì‹œí‚¨ ëª¨ë¸
- T5 : Transformer Encoder-Decoder í†µí•© LM
![](../../assets/images/week9/week9_day5_t.PNG)
  - ì—¬ëŸ¬ ê°œì˜ ì–´ì ˆì„ í•˜ë‚˜ì˜ mask tokenìœ¼ë¡œ ì¹˜í™˜
- Meena : ëŒ€í™” ëª¨ë¸ì„ ìœ„í•œ LM
![](../../assets/images/week9/week9_day5_m.PNG)
- Plog and Play Language Model(PPLM; Controllable LM) : ë‹¤ìŒì— ë“±ì¥í•  ë‹¨ì–´ë¥¼ í™•ë¥  ë¶„í¬ë¥¼ í†µí•´ ì„ íƒí•˜ëŠ”ë°, ì´ ë•Œ ë‚´ê°€ ì›í•˜ëŠ” ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ ì´ì „ ìƒíƒœì˜ vectorë¥¼ ìˆ˜ì • / ìˆ˜ì •ëœ vectorë¥¼ í†µí•´ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
![](../../assets/images/week9/week9_day5_p.PNG)
  - ì›í•˜ëŠ” ë‹¨ì–´ë“¤ì„ bag of wordì— ì €ì¥
  - bag of wordê°€ ìµœëŒ€ í™•ë¥ ì´ ë  ìˆ˜ ìˆë„ë¡ back propationì„ í†µí•´ chickenì—ì„œ ë‚˜ì˜¨ vector ì •ë³´ë¥¼ ìˆ˜ì •, Gradient ê°’ì„ Updateí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¨ìˆœíˆ ì´ì „ vectorê°’ì„ ìˆ˜ì •
- LXMERT(Cross-modal reasoning language model) : ì´ë¯¸ì§€ì™€ ìì—°ì–´ë¥¼ ë™ì‹œì— í•™ìŠµ
![](../../assets/images/week9/week9_day5_l.PNG)
- ViLBERT : BERT for vision-and-language
![](../../assets/images/week9/week9_day5_v.PNG)
  - ë¬¸ì¥ 1, ë¬¸ì¥ 2ì˜ ì…ë ¥ ëŒ€ì‹  ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„°ì™€ ìì—°ì–´ ì„ë² ë”© ë²¡í„°ê°€ ì…ë ¥ì´ ëœë‹¤.
- Dall-e : ìì—°ì–´ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë‚´ëŠ” ëª¨ë¸
  - VQ-VAEë¥¼ í†µí•´ ì´ë¯¸ì§€ì˜ ì°¨ì› ì¶•ì†Œí•˜ì—¬ í•™ìŠµ
  - Autoregressive í˜•íƒœë¡œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” í˜•íƒœë¡œ í•™ìŠµ
  - Text í† í°ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°”ì„ ë•Œ ì´ë¯¸ì§€ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµ
<br/>

## 2. ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš© / ê³ ë¯¼í•œ ë‚´ìš© (ê°•ì˜, ê³¼ì œ, í€´ì¦ˆ)
- **ì‹¤ìŠµì½”ë“œ : (9ê°•) GPT ê¸°ë°˜ ìì—°ì–´ ìƒì„± ëª¨ë¸ í•™ìŠµ - 0_ìì—°ì–´_ìƒì„±ë²•**
- **ì‹¤ìŠµì½”ë“œ : (9ê°•) GPT ê¸°ë°˜ ìì—°ì–´ ìƒì„± ëª¨ë¸ í•™ìŠµ - 1_Few_shot_learning**
- **ì‹¤ìŠµì½”ë“œ : (9ê°•) GPT ê¸°ë°˜ ìì—°ì–´ ìƒì„± ëª¨ë¸ í•™ìŠµ - 2_KoGPT_2_ê¸°ë°˜ì˜_ì±—ë´‡ã…‹**

## 3. ì°¸ê³ í•  ë§Œí•œ ìë£Œ
- **Reference**
  - chitchat
    - [1. Open-Dialog Chatbots for Learning New Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)
    - [2. Week 31 - ì´ë£¨ë‹¤ ê°™ì€ ì±—ë´‡ì€ ì–´ë–¤ ì›ë¦¬ë¡œ ì‘ë™í•˜ëŠ” ê±¸ê¹Œ?](https://jiho-ml.com/weekly-nlp-31/?fbclid=IwAR2iAaKug8rOBvRY6F7JHuSguBaygnR24XX9BBlLGdL5zZSeqgGASlvTQmk)
  - GPT-2 Classification
    - [1. ğŸ± GPT2 For Text Classification using Hugging Face ğŸ¤— Transformers](https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832)
  - Generate dataset from language model
    - [1. Generating Datasets with Pretrained Language Models](https://arxiv.org/abs/2104.07540?fbclid=IwAR21VeEUdy9LVaI3hz5nxhCRV7ZWiwKXwxGuyLgrj9o1DBOjhoUi2uedQ4A)
  - Reformer
    - [1. The Reformer - Pushing the limits of language modeling](https://colab.research.google.com/github/patrickvonplaten/blog/blob/master/notebooks/03_reformer.ipynb#scrollTo=mLMgZt_38dtR)
    - [2. PyTorch Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)
    - [3. Reformer For Masked LM](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)
    - [4. Training RoBERTa from scratch the missing guide polish language model](https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/index.html)
  - T-5
    - [1. TF-T5-text-to-text](https://github.com/snapthat/TF-T5-text-to-text)
    - [2. Text Generation with blurr](https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb)
    - [3. Transformers Summarization wandb](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)
    - [4. MT5 Inference for question generation](https://www.kaggle.com/parthplc/mt5-inference-for-question-generation)
    - [5. Fine-tune MT5 for question generation in hindi](https://www.kaggle.com/parthplc/finetune-mt5-for-question-generation-in-hindi)
  - Roberta
    - [1. Convert Model to Long](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)
    - [2. How to train a new language model from scratch using Transformers and](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw)
    - [3. Warm-starting RoBERTaShared for BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)
  - Longformer
    - [1. Longoemer QA Training](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb)
  - Multimodal transformers
    - [1. SimpleTransformers](https://github.com/ThilinaRajapakse/simpletransformers/blob/master/README.md#encoder-decoder)
    - [2. HuggingFace Transformersì™€ í…Œì´ë¸” í˜•ì‹ ë°ì´í„°ë¥¼ í†µí•©í•˜ëŠ” ë°©ë²•](https://ichi.pro/ko/huggingface-transformerswa-teibeul-hyeongsig-deiteoleul-tonghabhaneun-bangbeob-201256872169396)

## 4. í”¼ì–´ì„¸ì…˜
- ì „ì²´ì ì¸ ì½”ë“œ ë¦¬ë·° í›„ ë³‘í•©
- ìì„¸í•œ ë‚´ìš©ì€ [Peer Session](https://diagnostic-offer-ddb.notion.site/10-01-e76fc398bf404f0e88fcfd8fc0f268e2) ì°¸ì¡°

---
---